{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import math\n",
    "\n",
    "from DNA_Encoding import DNA_Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "expName = \"all_run_NewDataset\"\n",
    "outPath = \"Generated\"\n",
    "\n",
    "modelNames = [\"DLNN_3\", \"DLNN_5\", \"DLNN_CORENup\", \"Random_Forest\"]\n",
    "\n",
    "epochs=200\n",
    "batch_size = 64\n",
    "shuffle = False\n",
    "seed = None\n",
    "\n",
    "dataset_path = \"New_Dataset\"\n",
    "setting = \"New_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 22166\n",
      "Testing samples: 6928\n",
      "Evaluation samples: 5542\n",
      "Datapoint shape: (99, 21)\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### Load Training, Testing and Evaluation datasets\n",
    "##################################################################################\n",
    "\n",
    "## training data for the models\n",
    "train_X = np.load(\"New_Dataset\\\\New_dataset\\\\train_X.npy\")\n",
    "train_y = np.load(\"New_Dataset\\\\New_dataset\\\\train_y.npy\")\n",
    "\n",
    "## validation dataset for the models\n",
    "eval_X = np.load(\"New_Dataset\\\\New_dataset\\\\eval_X.npy\")\n",
    "eval_y = np.load(\"New_Dataset\\\\New_dataset\\\\eval_y.npy\")\n",
    "\n",
    "## testing dataset for the models\n",
    "test_X = np.load(\"New_Dataset\\\\New_dataset\\\\test_X.npy\")\n",
    "test_y = np.load(\"New_Dataset\\\\New_dataset\\\\test_y.npy\")\n",
    "\n",
    "print(\"Training samples:\",train_X.shape[0])\n",
    "print(\"Testing samples:\",test_X.shape[0])\n",
    "print(\"Evaluation samples:\",eval_X.shape[0])\n",
    "\n",
    "print(\"Datapoint shape:\",(train_X.shape[1], train_X.shape[2]))\n",
    "\n",
    "# Data is already one-hot-encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_1d = np.array([xi[0] for xi in train_y])\n",
    "eval_y_1d = np.array([xi[0] for xi in eval_y])\n",
    "test_y_1d = np.array([xi[0] for xi in test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(np.clip(y_pred, 0, 1))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to generate the DLNN-X and CORENup network architectures with parameters\n",
    "##################################################################################\n",
    "\n",
    "def Conv_LSTM_DLNN(input_shape=(150,4), conv_filters_per_layer = 50, kernel_length = 5, lstm_decode_units = 50, \n",
    "                   learn_rate = 0.0003, prob = 0.5, loss = 'binary_crossentropy', metrics = None, max_pool_width = 2, \n",
    "                   max_pool_stride = 2, dense_decode_units = 150):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv1D(conv_filters_per_layer, kernel_length, input_shape = input_shape, \n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(beta), padding=\"same\"))\n",
    "    \n",
    "    model.add(tf.keras.layers.MaxPool1D(pool_size = max_pool_width, strides = max_pool_stride))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(prob))\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "                                   kernel_regularizer = tf.keras.regularizers.l2(beta), dropout = 0.1))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(prob))\n",
    "\n",
    "    model.add(tf.keras .layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(dense_decode_units, kernel_regularizer=tf.keras.regularizers.l2(beta), activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(prob))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(beta), activation='sigmoid'))\n",
    "    \n",
    "    #[tf.keras.metrics.binary_accuracy, metrics.precision, metrics.recall, metrics.f1score])\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss, metrics = metrics) \n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def DLNN_CORENup(input_shape = (150,4),\n",
    "                 conv_filters_per_layer_1 = 50, kernel_length_1 = 5, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "                 max_pool_width_1 = 2, max_pool_stride_1 = 2, ## 1st Maxpool layer parameters\n",
    "                 lstm_decode_units = 50, ## LSTM layer parameters\n",
    "                 conv_filters_per_layer_2 = 50,  kernel_length_2 = 10, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "                 max_pool_width_2 = 2, max_pool_stride_2 = 2, ## 2nd Maxpool layer parameters\n",
    "                 dense_decode_units = 370, ## Dense layer parameters\n",
    "                 prob = 0.5, learn_rate = 0.0003, loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape = input_shape)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1, input_shape = input_shape, \n",
    "                                strides = conv_strides_1, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = \"same\")(input1)\n",
    "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "    x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              dropout=0.1)(x1)\n",
    "    x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, strides = conv_strides_2, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), padding = 'same')(x1)\n",
    "    x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "    x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "\n",
    "    ## Fully connected Layers\n",
    "\n",
    "    y = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "    y1 = tf.keras.layers.Dense(dense_decode_units, kernel_regularizer = tf.keras.regularizers.l2(beta), activation = 'relu')(y)\n",
    "    \n",
    "    y1 = tf.keras.layers.Dropout(prob)(y1)\n",
    "    \n",
    "    y1 = tf.keras.layers.Dense(1, kernel_regularizer = tf.keras.regularizers.l2(beta), activation = 'sigmoid')(y1)\n",
    "\n",
    "    ## Generate Model from input and output\n",
    "    model = tf.keras.models.Model(inputs=[input1], outputs=y1)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr=learn_rate), loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### For each model, train and generate evaluation metrics\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Kernel_Length\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "input_shape = train_X[0].shape\n",
    "\n",
    "for modelName in modelNames:\n",
    "    if modelName == \"Random_Forest\"\n",
    "    \n",
    "    else:\n",
    "        if modelName == \"DLNN_CORENup\":\n",
    "            kernel_length = \"5,10\"\n",
    "            model = DLNN_CORENup(input_shape = input_shape)\n",
    "\n",
    "        else:\n",
    "            kernel_length = int(modelName[-1])\n",
    "            model = Conv_LSTM_DLNN(input_shape = input_shape, conv_filters_per_layer = 50, kernel_length = kernel_length, \n",
    "                                   lstm_decode_units = 50, learn_rate = 0.0003, prob = 0.5, loss='binary_crossentropy', \n",
    "                                   metrics=None)\n",
    "\n",
    "        modelPath = os.path.join(outPath, expName, \"Models\")\n",
    "        if(not os.path.isdir(modelPath)):\n",
    "            os.makedirs(modelPath)\n",
    "\n",
    "        print(\"===============================================================================================\")\n",
    "        print(\"Model\", modelName, \"initialized.\")\n",
    "\n",
    "        ## Define the model callbacks for early stopping and saving the model. Then train model \n",
    "        modelCallbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(os.path.join(modelPath, \"{}.hdf5\".format(modelName)),\n",
    "                                               monitor = 'val_loss', verbose = 0, save_best_only = True, \n",
    "                                               save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 0, \n",
    "                                             mode = 'auto', baseline = None, restore_best_weights = False)\n",
    "        ]\n",
    "        model.fit(x = train_X, y = train_y_1d, batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "                  callbacks = modelCallbacks, validation_data = (eval_X, eval_y_1d))\n",
    "\n",
    "        print(\"\\nModel\", modelName, \"Trained.\")\n",
    "        ##################################################################################\n",
    "        ##### Prediction and metrics for TRAIN dataset\n",
    "        ##################################################################################\n",
    "\n",
    "        y_pred = model.predict(train_X)\n",
    "        label_pred = pred2label(y_pred)\n",
    "        # Compute precision, recall, sensitivity, specifity, mcc\n",
    "        acc = accuracy_score(train_y_1d, label_pred)\n",
    "        prec = precision_score(train_y_1d,label_pred)\n",
    "\n",
    "        conf = confusion_matrix(train_y_1d, label_pred)\n",
    "        if(conf[0][0]+conf[1][0]):\n",
    "            sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "        else:\n",
    "            sens = 0.0\n",
    "        if(conf[1][1]+conf[0][1]):\n",
    "            spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "        else:\n",
    "            spec = 0.0\n",
    "        if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "            mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "        else:\n",
    "            mcc= 0.0\n",
    "        fpr, tpr, thresholds = roc_curve(train_y_1d, y_pred)\n",
    "        auc = roc_auc_score(train_y_1d, y_pred)\n",
    "\n",
    "        evaluations[\"Model\"].append(modelName)\n",
    "        evaluations[\"Kernel_Length\"].append(kernel_length)\n",
    "        evaluations[\"Dataset\"].append(\"Train\")\n",
    "        evaluations[\"Accuracy\"].append(acc)\n",
    "        evaluations[\"Precision\"].append(prec)\n",
    "        evaluations[\"TPR\"].append(tpr)\n",
    "        evaluations[\"FPR\"].append(fpr)\n",
    "        evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "        evaluations[\"AUC\"].append(auc)\n",
    "        evaluations[\"Sensitivity\"].append(sens)\n",
    "        evaluations[\"Specificity\"].append(spec)\n",
    "        evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "        print(\"Prediction and Evaluation on TRAIN dataset done for\",modelName,\"model.\")\n",
    "\n",
    "        ##################################################################################\n",
    "        ##### Prediction and metrics for EVAL dataset\n",
    "        ##################################################################################\n",
    "\n",
    "        y_pred = model.predict(eval_X)\n",
    "        label_pred = pred2label(y_pred)\n",
    "        # Compute precision, recall, sensitivity, specifity, mcc\n",
    "        acc = accuracy_score(eval_y_1d, label_pred)\n",
    "        prec = precision_score(eval_y_1d,label_pred)\n",
    "\n",
    "        conf = confusion_matrix(eval_y_1d, label_pred)\n",
    "        if(conf[0][0]+conf[1][0]):\n",
    "            sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "        else:\n",
    "            sens = 0.0\n",
    "        if(conf[1][1]+conf[0][1]):\n",
    "            spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "        else:\n",
    "            spec = 0.0\n",
    "        if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "            mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "        else:\n",
    "            mcc= 0.0\n",
    "        fpr, tpr, thresholds = roc_curve(eval_y_1d, y_pred)\n",
    "        auc = roc_auc_score(eval_y_1d, y_pred)\n",
    "\n",
    "        evaluations[\"Model\"].append(modelName)\n",
    "        evaluations[\"Kernel_Length\"].append(kernel_length)\n",
    "        evaluations[\"Dataset\"].append(\"Eval\")\n",
    "        evaluations[\"Accuracy\"].append(acc)\n",
    "        evaluations[\"Precision\"].append(prec)\n",
    "        evaluations[\"TPR\"].append(tpr)\n",
    "        evaluations[\"FPR\"].append(fpr)\n",
    "        evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "        evaluations[\"AUC\"].append(auc)\n",
    "        evaluations[\"Sensitivity\"].append(sens)\n",
    "        evaluations[\"Specificity\"].append(spec)\n",
    "        evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "        print(\"Prediction and Evaluation on EVAL dataset done for\",modelName,\"model.\")\n",
    "\n",
    "        ##################################################################################\n",
    "        ##### Prediction and metrics for TEST dataset\n",
    "        ##################################################################################\n",
    "\n",
    "        y_pred = model.predict(test_X)\n",
    "        label_pred = pred2label(y_pred)\n",
    "        # Compute precision, recall, sensitivity, specifity, mcc\n",
    "        acc = accuracy_score(test_y_1d, label_pred)\n",
    "        prec = precision_score(test_y_1d,label_pred)\n",
    "\n",
    "        conf = confusion_matrix(test_y_1d, label_pred)\n",
    "        if(conf[0][0]+conf[1][0]):\n",
    "            sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "        else:\n",
    "            sens = 0.0\n",
    "        if(conf[1][1]+conf[0][1]):\n",
    "            spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "        else:\n",
    "            spec = 0.0\n",
    "        if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "            mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "        else:\n",
    "            mcc= 0.0\n",
    "        fpr, tpr, thresholds = roc_curve(test_y_1d, y_pred)\n",
    "        auc = roc_auc_score(test_y_1d, y_pred)\n",
    "\n",
    "        evaluations[\"Model\"].append(modelName)\n",
    "        evaluations[\"Kernel_Length\"].append(kernel_length)\n",
    "        evaluations[\"Dataset\"].append(\"Test\")\n",
    "        evaluations[\"Accuracy\"].append(acc)\n",
    "        evaluations[\"Precision\"].append(prec)\n",
    "        evaluations[\"TPR\"].append(tpr)\n",
    "        evaluations[\"FPR\"].append(fpr)\n",
    "        evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "        evaluations[\"AUC\"].append(auc)\n",
    "        evaluations[\"Sensitivity\"].append(sens)\n",
    "        evaluations[\"Specificity\"].append(spec)\n",
    "        evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "        print(\"Prediction and Evaluation on TEST dataset done for\",modelName,\"model.\")\n",
    "\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "##################################################################################\n",
    "##### Dump evaluations to a file\n",
    "##################################################################################\n",
    "\n",
    "evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "if(not os.path.isdir(evalPath)):\n",
    "    os.makedirs(evalPath)\n",
    "\n",
    "pickle.dump(evaluations,\n",
    "            open(os.path.join(evalPath, \"_Evaluation_All_Models_Datasets.pickle\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Add import statement here, to make this next part of code standalone executable\n",
    "##################################################################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Parameters used only in this section\n",
    "##################################################################################\n",
    "\n",
    "expName = \"Test_Run_New_Dataset\"\n",
    "outPath = \"Generated\"\n",
    "\n",
    "modelNames = [\"DLNN_3\", \"DLNN_5\", \"DLNN_CORENup\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Load file and convert to dataframe for easy manipulation\n",
    "##################################################################################\n",
    "\n",
    "evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "if(not os.path.isdir(evalPath)):\n",
    "    os.makedirs(evalPath)\n",
    "\n",
    "evaluations = pickle.load(open(os.path.join(evalPath, \"_Evaluation_All_Models_Datasets.pickle\"), \"rb\"))\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Group dataset (mean of metrics) by [Dataset, Model, Train_Test] combinations\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Model\", \n",
    "                                                 \"Dataset\"]).mean().filter(['Accuracy',\n",
    "                                                                          'Precision', \n",
    "                                                                          'AUC', \n",
    "                                                                          'Sensitivity', \n",
    "                                                                          'Specificity', \n",
    "                                                                          'MCC'])\n",
    "\n",
    "DLNN_3_DF = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(0), ['DLNN_3'])]\n",
    "DLNN_5_DF = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(0), ['DLNN_5'])]\n",
    "DLNN_CORENup_DF = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(0), ['DLNN_CORENup'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLNN_CORENup_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a metric to plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"Accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLNN_CORENup_DF.index.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Visualize with a multiple Bar chart\n",
    "##################################################################################\n",
    "\n",
    "x = np.arange(len(DLNN_CORENup_DF[metric_to_plot]))\n",
    "width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17,6))\n",
    "rects1 = ax.bar(x - (2.5*(width/2)), round(DLNN_3_DF[metric_to_plot]*100, 3), width, label='DLNN_3')\n",
    "rects2 = ax.bar(x + (0*(width/2)), round(DLNN_5_DF[metric_to_plot]*100, 3), width, label='DLNN_5')\n",
    "rects3 = ax.bar(x + (2.5*(width/2)), round(DLNN_CORENup_DF[metric_to_plot]*100, 3), width, label='DLNN_CORENup')\n",
    "\n",
    "## Custom y-axis tick labels\n",
    "ax.set_ylabel(metric_to_plot)\n",
    "ax.set_ylim([(math.floor(min(evaluations_df_grouped[metric_to_plot])*10)-1)*10, \n",
    "            (math.ceil(max(evaluations_df_grouped[metric_to_plot])*10)+1)*10])\n",
    "# ax.set_ylim([80, 105])\n",
    "\n",
    "## Custom x-axis tick labels\n",
    "ax.set_xticks(x)\n",
    "# ax.set_xticklabels(DLNN_3_Train.index.get_level_values(0))\n",
    "# ax.set_xticklabels([m+\" - \"+str(n) for m,n in \n",
    "#                         zip(DLNN_CORENup_Train.index.get_level_values(0),DLNN_CORENup_Train.index.get_level_values(1))],\n",
    "#                   rotation=30)\n",
    "ax.set_xticklabels(DLNN_CORENup_DF.index.get_level_values(1),\n",
    "                  rotation=30)\n",
    "\n",
    "\n",
    "ax.set_title(metric_to_plot+' by Dataset and Model')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\", \n",
    "                    ha='center', va='bottom', rotation=90)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all metrics' plots to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Iteratively generate comparison plot using every metric\n",
    "##################################################################################\n",
    "\n",
    "for metric_to_plot in list(evaluations_df_grouped.columns):\n",
    "    \n",
    "    x = np.arange(len(DLNN_CORENup_DF[metric_to_plot]))\n",
    "    width = 0.15\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(17,6))\n",
    "    rects1 = ax.bar(x - (2.5*(width/2)), round(DLNN_3_DF[metric_to_plot]*100, 3), width, label='DLNN_3')\n",
    "    rects2 = ax.bar(x + (0*(width/2)), round(DLNN_5_DF[metric_to_plot]*100, 3), width, label='DLNN_5')\n",
    "    rects3 = ax.bar(x + (2.5*(width/2)), round(DLNN_CORENup_DF[metric_to_plot]*100, 3), width, label='DLNN_CORENup')\n",
    "\n",
    "    ## Custom y-axis tick labels\n",
    "    ax.set_ylabel(metric_to_plot)\n",
    "    ax.set_ylim([(math.floor(min(evaluations_df_grouped[metric_to_plot])*10)-1)*10, \n",
    "                (math.ceil(max(evaluations_df_grouped[metric_to_plot])*10)+1)*10])\n",
    "    # ax.set_ylim([80, 105])\n",
    "\n",
    "    ## Custom x-axis tick labels\n",
    "    ax.set_xticks(x)\n",
    "    # ax.set_xticklabels(DLNN_3_Train.index.get_level_values(0))\n",
    "    # ax.set_xticklabels([m+\" - \"+str(n) for m,n in \n",
    "    #                         zip(DLNN_CORENup_Train.index.get_level_values(0),DLNN_CORENup_Train.index.get_level_values(1))],\n",
    "    #                   rotation=30)\n",
    "    ax.set_xticklabels(DLNN_CORENup_DF.index.get_level_values(1),\n",
    "                      rotation=30)\n",
    "\n",
    "\n",
    "    ax.set_title(metric_to_plot+' by Dataset and Model')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\", \n",
    "                        ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    autolabel(rects3)\n",
    "    \n",
    "    plt.savefig(os.path.join(evalPath, \"{}_Comparison\".format(metric_to_plot)))\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
