{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# from Bio import SeqIO\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# import xgboost as xgb\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "import math\n",
    "\n",
    "from lncRNAmiRNA import DForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all experiment parameters\n",
    "##################################################################################\n",
    "\n",
    "expName = \"MathFeature_setting1\"\n",
    "outPath = \"Generated\"\n",
    "\n",
    "dataset_path = \"Datasets\"\n",
    "setting = \"Setting1\"\n",
    "output_path = \"Results_ALL_cosineSimilarityAE_DeepForest\"\n",
    "\n",
    "datafile_extensions = \".csv\"\n",
    "\n",
    "modelNames = [\"DeepForest\"]\n",
    "\n",
    "shuffle = True\n",
    "seed = 123\n",
    "\n",
    "reject_encoding_list = [\"ALL\", \"NM-complex\", \"kmer\", \"kstep\", \"rckmer\"]\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "##################################################################################\n",
    "##### Define the modelling hyperparameters\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Checking the directory\n",
    "##################################################################################\n",
    "\n",
    "dataset_setting_path = os.path.join(outPath, expName, dataset_path, setting)\n",
    "dataset_varieties = next(os.walk(dataset_setting_path))\n",
    "result_output_path = os.path.join(outPath, expName, output_path, setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(np.clip(y_pred, 0, 1))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoEncoder_1D(input_length,\n",
    "                   latent_space_size = 500,\n",
    "                   learn_rate = 0.001, \n",
    "                   loss = 'mean_squared_error', \n",
    "                   metrics = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Simple Autoencoder, full connected network.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tf.keras.Input(shape = input_length)\n",
    "    \n",
    "    # encoded = Dense(units = latent_space_size*2, activation='relu')(inputs)\n",
    "    \n",
    "    # encoder = tf.keras.layers.Dense(units = latent_space_size)(inputs)\n",
    "    # decoder = tf.keras.layers.Dense(units = input_length)(encoder)\n",
    "    \n",
    "    enc1 = tf.keras.layers.Dense(units = int(input_length/2))(inputs)\n",
    "    encoder = tf.keras.layers.Dense(units = latent_space_size)(enc1)\n",
    "    dec1 = tf.keras.layers.Dense(units = int(input_length/2))(encoder)\n",
    "    decoder = tf.keras.layers.Dense(units = input_length)(dec1)\n",
    "    \n",
    "    # encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "    # decoded = Dense(328, activation='sigmoid')(encoded)\n",
    "    \n",
    "    encoder = tf.keras.Model(inputs, encoder)\n",
    "    autoencoder = tf.keras.Model(inputs, decoder)\n",
    "    \n",
    "    # autoencoder.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss)\n",
    "    # autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    autoencoder.compile(optimizer = tf.keras.optimizers.Adadelta(lr = learn_rate), loss = loss)\n",
    "    \n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation by joining from all encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_variety in dataset_varieties[1]:\n",
    "#     current_dataset_variety_path = os.path.join(dataset_setting_path, dataset_variety)\n",
    "    \n",
    "    \n",
    "#     print(\"\\nProcessing variety: \", dataset_variety)\n",
    "#     print(\"======================================\")\n",
    "    \n",
    "#     i = 0\n",
    "    \n",
    "#     for root, dirs, files in os.walk(current_dataset_variety_path):\n",
    "#         for file in files:\n",
    "#             if os.path.splitext(file)[-1] == datafile_extensions:\n",
    "                \n",
    "#                 encoding_type = file.split(\".\")[0].split(\"_\")[-1]\n",
    "                \n",
    "#                 if encoding_type not in reject_encoding_list:\n",
    "                \n",
    "#                     input_file_full_path = os.path.join(root, file)\n",
    "\n",
    "#                     ## check if input file has header\n",
    "#                     file_obj = open(input_file_full_path, \"r\")\n",
    "#                     first_line = file_obj.readline()\n",
    "#                     file_obj.close()\n",
    "#                     file_has_header = None\n",
    "#                     if first_line.split(\",\")[0] == \"nameseq\" or first_line.replace(\"\\n\", \"\").split(\",\")[-1] == \"label\":\n",
    "#                         file_has_header = 0\n",
    "\n",
    "#                     sequences_df = pd.read_csv(input_file_full_path, header = file_has_header)\n",
    "                    \n",
    "#                     ## adding encoding type to header names\n",
    "#                     cols = list(sequences_df.columns)\n",
    "#                     cols = [encoding_type+\"_\"+str(col) for col in cols]\n",
    "#                     sequences_df.columns = cols\n",
    "                    \n",
    "#                     # sequences_df = pd.read_csv(input_file_full_path, header = \"infer\")\n",
    "#                     # print(\"Encoding completed adding: \", encoding_type)\n",
    "#                     # print(\"Columns: \", sequences_df.columns)\n",
    "                    \n",
    "#                     sequences_df = sequences_df.rename(columns = {sequences_df.columns[0] : 'nameseq', \n",
    "#                                                                   sequences_df.columns[-1] : 'label'})\n",
    "                    \n",
    "#                     sequences_df = sequences_df.drop(\"label\", axis = 1)\n",
    "                    \n",
    "#                     if i == 0:\n",
    "#                         variety_dataset_df = sequences_df\n",
    "                        \n",
    "#                     else:\n",
    "#                         # variety_dataset_df = pd.concat([variety_dataset_df, sequences_df], \n",
    "#                         #                                join=\"inner\", \n",
    "#                         #                                keys = (\"nameseq\"), \n",
    "#                         #                                ignore_index = True, \n",
    "#                         #                                axis = 1)\n",
    "#                         # variety_dataset_df = variety_dataset_df.rename(columns = {0 : 'nameseq'})\n",
    "                        \n",
    "#                         variety_dataset_df = pd.merge(variety_dataset_df, sequences_df, \n",
    "#                                                       left_on='nameseq', right_on='nameseq', \n",
    "#                                                       how='inner')\n",
    "                    \n",
    "#                     print(\"Encoding completed adding: \", encoding_type)\n",
    "                    \n",
    "#                     i = i+1\n",
    "                    \n",
    "#     file_name = \"_\".join(file.split(\".\")[0].split(\"_\")[0:-1]+[\"ALL\"])+\".\"+file.split(\".\")[1]\n",
    "#     variety_dataset_df.to_csv(os.path.join(root, file_name), \n",
    "#                               header = True, \n",
    "#                               index = False)\n",
    "    \n",
    "#     print(\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan/Infinity correction - dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for root, dirs, files in os.walk(dataset_setting_path):\n",
    "#     for file in files:\n",
    "#         if (os.path.splitext(file)[-1] == datafile_extensions) & (file.split(\".\")[0].split(\"_\")[-1] == \"ALL\"):\n",
    "            \n",
    "#             input_file_full_path = os.path.join(root, file)\n",
    "            \n",
    "#             print(\" Processing file : \",input_file_full_path)\n",
    "            \n",
    "#             sequences_df = pd.read_csv(input_file_full_path, header = \"infer\", low_memory=False)\n",
    "            \n",
    "#             vals = sequences_df.drop(\"nameseq\", axis = 1).values.astype(np.float)\n",
    "#             sequences_df_columns = sequences_df.drop(\"nameseq\", axis = 1).columns\n",
    "\n",
    "#             if np.isnan(np.max(vals)):\n",
    "#                 nans = np.argwhere(np.isnan(vals.astype(np.float)))\n",
    "#                 vals = np.delete(vals, np.unique(nans[:,1]), axis=1)\n",
    "#                 drop_cols = sequences_df_columns[np.unique(nans[:,1])]\n",
    "#                 for drop_col in drop_cols:\n",
    "#                     sequences_df = sequences_df.drop(drop_col, axis = 1)\n",
    "\n",
    "#             sequences_df_columns = sequences_df.drop(\"nameseq\", axis = 1).columns\n",
    "\n",
    "#             if np.max(vals)>1e+308:\n",
    "#                 vals = np.where(vals == np.max(vals), \n",
    "#                                 None, \n",
    "#                                 vals)\n",
    "#                 nans = np.argwhere(np.isnan(vals.astype(np.float)))\n",
    "#                 vals = np.delete(vals, np.unique(nans[:,1]), axis=1)\n",
    "#                 drop_cols = sequences_df_columns[np.unique(nans[:,1])]\n",
    "#                 for drop_col in drop_cols:\n",
    "#                     sequences_df = sequences_df.drop(drop_col, axis = 1)\n",
    "                    \n",
    "#             sequences_df.to_csv(input_file_full_path.replace(\"_ALL\", \"_ALL-v2\"), \n",
    "#                                 header = True, \n",
    "#                                 index = False)\n",
    "            \n",
    "#             print(\" Generated file : \",input_file_full_path.replace(\"_ALL\", \"_ALL-v2\"))\n",
    "#             print(\"\\n################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "File: Generated\\MathFeature_setting1\\Datasets\\Setting1\\Drosophila\\nucleosomes_vs_linkers_melanogaster_ALL-v2.csv\n",
      "Nucleosomi: 2900\n",
      "Linker: 2850\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 1s 13ms/step - loss: -0.8383 - val_loss: -0.9918\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9935 - val_loss: -0.9929\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9938 - val_loss: -0.9941\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9947 - val_loss: -0.9944\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9948 - val_loss: -0.9945\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9951 - val_loss: -0.9948\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9954 - val_loss: -0.9952\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9957 - val_loss: -0.9955\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9961 - val_loss: -0.9959\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9962 - val_loss: -0.9963\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9967 - val_loss: -0.9965\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9969 - val_loss: -0.9969\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9972 - val_loss: -0.9972\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9975 - val_loss: -0.9975\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9978 - val_loss: -0.9977\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9980 - val_loss: -0.9980\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9983 - val_loss: -0.9982\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9984 - val_loss: -0.9984\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9986 - val_loss: -0.9987\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9988 - val_loss: -0.9988\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9990 - val_loss: -0.9990\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9991 - val_loss: -0.9991\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9992 - val_loss: -0.9992\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9993 - val_loss: -0.9993\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9993 - val_loss: -0.9994\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9995 - val_loss: -0.9995\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9995 - val_loss: -0.9995\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: -0.9996 - val_loss: -0.9996\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9996 - val_loss: -0.9996\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9997 - val_loss: -0.9998\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -0.9999 - val_loss: -1.0000\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 1s 8ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "\n",
      "Train/Test model DeepForest on Fold #0.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6641304347826087\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6543478260869565\n",
      "\n",
      "Train/Test model DeepForest on Fold #1.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6684782608695652\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6684782608695652\n",
      "\n",
      "Train/Test model DeepForest on Fold #2.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6673913043478261\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6695652173913044\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.6782608695652174\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.6847826086956522\n",
      "Adding/Training Layer, n_layer=5\n",
      "Layer validation accuracy = 0.6739130434782609\n",
      "\n",
      "Train/Test model DeepForest on Fold #3.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6619565217391304\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6641304347826087\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.6608695652173913\n",
      "\n",
      "Train/Test model DeepForest on Fold #4.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6608695652173913\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6554347826086957\n",
      "\n",
      "======================================================================\n",
      "\n",
      "File: Generated\\MathFeature_setting1\\Datasets\\Setting1\\Elegans\\nucleosomes_vs_linkers_elegans_ALL-v2.csv\n",
      "Nucleosomi: 2567\n",
      "Linker: 2608\n",
      "Epoch 1/100\n",
      "65/65 [==============================] - 1s 11ms/step - loss: -0.8204 - val_loss: -0.9921\n",
      "Epoch 2/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9950 - val_loss: -0.9944\n",
      "Epoch 3/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9956 - val_loss: -0.9948\n",
      "Epoch 4/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9960 - val_loss: -0.9954\n",
      "Epoch 5/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9962 - val_loss: -0.9957\n",
      "Epoch 6/100\n",
      "65/65 [==============================] - 0s 6ms/step - loss: -0.9965 - val_loss: -0.9956\n",
      "Epoch 7/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9965 - val_loss: -0.9957\n",
      "Epoch 8/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9967 - val_loss: -0.9960\n",
      "Epoch 9/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9968 - val_loss: -0.9961\n",
      "Epoch 10/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9968 - val_loss: -0.9963\n",
      "Epoch 11/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9971 - val_loss: -0.9964\n",
      "Epoch 12/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9970 - val_loss: -0.9965\n",
      "Epoch 13/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9972 - val_loss: -0.9966\n",
      "Epoch 14/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9973 - val_loss: -0.9968\n",
      "Epoch 15/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9974 - val_loss: -0.9969\n",
      "Epoch 16/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9976 - val_loss: -0.9971\n",
      "Epoch 17/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9976 - val_loss: -0.9971\n",
      "Epoch 18/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9978 - val_loss: -0.9974\n",
      "Epoch 19/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9979 - val_loss: -0.9974\n",
      "Epoch 20/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9980 - val_loss: -0.9976\n",
      "Epoch 21/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9981 - val_loss: -0.9977\n",
      "Epoch 22/100\n",
      "65/65 [==============================] - 1s 11ms/step - loss: -0.9982 - val_loss: -0.9979\n",
      "Epoch 23/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9983 - val_loss: -0.9980\n",
      "Epoch 24/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9984 - val_loss: -0.9981\n",
      "Epoch 25/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9985 - val_loss: -0.9982\n",
      "Epoch 26/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9986 - val_loss: -0.9983\n",
      "Epoch 27/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9987 - val_loss: -0.9985\n",
      "Epoch 28/100\n",
      "65/65 [==============================] - 1s 10ms/step - loss: -0.9987 - val_loss: -0.9985\n",
      "Epoch 29/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9988 - val_loss: -0.9986\n",
      "Epoch 30/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9989 - val_loss: -0.9987\n",
      "Epoch 31/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9990 - val_loss: -0.9988\n",
      "Epoch 32/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9990 - val_loss: -0.9989\n",
      "Epoch 33/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9991 - val_loss: -0.9990\n",
      "Epoch 34/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9992 - val_loss: -0.9991\n",
      "Epoch 35/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9992 - val_loss: -0.9991\n",
      "Epoch 36/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9993 - val_loss: -0.9992\n",
      "Epoch 37/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9994 - val_loss: -0.9993\n",
      "Epoch 38/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9994 - val_loss: -0.9993\n",
      "Epoch 39/100\n",
      "65/65 [==============================] - 1s 10ms/step - loss: -0.9994 - val_loss: -0.9994\n",
      "Epoch 40/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9995 - val_loss: -0.9994\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9995 - val_loss: -0.9995\n",
      "Epoch 42/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9996 - val_loss: -0.9995\n",
      "Epoch 43/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9996 - val_loss: -0.9995\n",
      "Epoch 44/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9996 - val_loss: -0.9996\n",
      "Epoch 45/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9997 - val_loss: -0.9996\n",
      "Epoch 46/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9997 - val_loss: -0.9996\n",
      "Epoch 47/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 48/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 49/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 50/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9997\n",
      "Epoch 51/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 52/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 53/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 54/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 55/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 56/100\n",
      "65/65 [==============================] - 1s 10ms/step - loss: -0.9999 - val_loss: -0.9998\n",
      "Epoch 57/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 58/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 59/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 60/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 61/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 62/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 63/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 64/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 65/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 66/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 67/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 68/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 69/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -0.9999\n",
      "Epoch 70/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 71/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 72/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 73/100\n",
      "65/65 [==============================] - 1s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 74/100\n",
      "65/65 [==============================] - 1s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 75/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 76/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 77/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 78/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 79/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 80/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 81/100\n",
      "65/65 [==============================] - 1s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 82/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 83/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 84/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 85/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 86/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 87/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 88/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 89/100\n",
      "65/65 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 90/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 91/100\n",
      "65/65 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 92/100\n",
      "65/65 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 93/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 94/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 95/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 96/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 97/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 98/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 99/100\n",
      "65/65 [==============================] - 1s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 100/100\n",
      "65/65 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "\n",
      "Train/Test model DeepForest on Fold #0.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7765700483091788\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.782608695652174\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7814009661835749\n",
      "\n",
      "Train/Test model DeepForest on Fold #1.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7801932367149759\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.785024154589372\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7777777777777778\n",
      "\n",
      "Train/Test model DeepForest on Fold #2.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7765700483091788\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7741545893719807\n",
      "\n",
      "Train/Test model DeepForest on Fold #3.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7971014492753623\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7934782608695652\n",
      "\n",
      "Train/Test model DeepForest on Fold #4.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7910628019323671\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7898550724637681\n",
      "\n",
      "======================================================================\n",
      "\n",
      "File: Generated\\MathFeature_setting1\\Datasets\\Setting1\\Homo_Sapiens\\nucleosomes_vs_linkers_sapiens_ALL-v2.csv\n",
      "Nucleosomi: 2273\n",
      "Linker: 2300\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.7996 - val_loss: -0.9901\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9931 - val_loss: -0.9956\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 0s 6ms/step - loss: -0.9952 - val_loss: -0.9953\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9954 - val_loss: -0.9961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9958 - val_loss: -0.9964\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9960 - val_loss: -0.9966\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9962 - val_loss: -0.9966\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9962 - val_loss: -0.9968\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9964 - val_loss: -0.9968\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9965 - val_loss: -0.9971\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9968 - val_loss: -0.9972\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9968 - val_loss: -0.9973\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9970 - val_loss: -0.9974\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.9971 - val_loss: -0.9975\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9972 - val_loss: -0.9975\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9974 - val_loss: -0.9978\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9975 - val_loss: -0.9979\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9977 - val_loss: -0.9980\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9977 - val_loss: -0.9981\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.9979 - val_loss: -0.9982\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9980 - val_loss: -0.9983\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9982 - val_loss: -0.9984\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.9982 - val_loss: -0.9985\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9984 - val_loss: -0.9986\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9985 - val_loss: -0.9987\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9986 - val_loss: -0.9988\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9987 - val_loss: -0.9989\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.9988 - val_loss: -0.9990\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9989 - val_loss: -0.9990\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9990 - val_loss: -0.9991\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.9990 - val_loss: -0.9991\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9991 - val_loss: -0.9992\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9991 - val_loss: -0.9993\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9992 - val_loss: -0.9993\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9993 - val_loss: -0.9994\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9993 - val_loss: -0.9994\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 1s 11ms/step - loss: -0.9994 - val_loss: -0.9995\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9994 - val_loss: -0.9995\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9995 - val_loss: -0.9995\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9995 - val_loss: -0.9996\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9995 - val_loss: -0.9996\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.9996 - val_loss: -0.9996\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9996 - val_loss: -0.9997\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9996 - val_loss: -0.9997\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 1s 11ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9997 - val_loss: -0.9998\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9998 - val_loss: -0.9999\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 1s 12ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 1s 11ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -1.0000\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -0.9999 - val_loss: -1.0000\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 1s 12ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 1s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "\n",
      "Train/Test model DeepForest on Fold #0.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6325136612021858\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6420765027322405\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.6420765027322405\n",
      "\n",
      "Train/Test model DeepForest on Fold #1.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6543715846994536\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6557377049180327\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.662568306010929\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.657103825136612\n",
      "\n",
      "Train/Test model DeepForest on Fold #2.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6229508196721312\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6338797814207651\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.6325136612021858\n",
      "\n",
      "Train/Test model DeepForest on Fold #3.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6407103825136612\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6407103825136612\n",
      "\n",
      "Train/Test model DeepForest on Fold #4.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6789617486338798\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6898907103825137\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.6871584699453552\n",
      "\n",
      "======================================================================\n",
      "\n",
      "File: Generated\\MathFeature_setting1\\Datasets\\Setting1\\Yeast\\nucleosomes_vs_linkers_yeast_ALL-v2.csv\n",
      "Nucleosomi: 1880\n",
      "Linker: 1740\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: -0.7414 - val_loss: -0.9835\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9858 - val_loss: -0.9885\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9883 - val_loss: -0.9893\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9888 - val_loss: -0.9900\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9897 - val_loss: -0.9908\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9911 - val_loss: -0.9919\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9918 - val_loss: -0.9927\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9929 - val_loss: -0.9937\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9938 - val_loss: -0.9945\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9947 - val_loss: -0.9956\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9955 - val_loss: -0.9960\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9964 - val_loss: -0.9970\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9970 - val_loss: -0.9973\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 1s 11ms/step - loss: -0.9974 - val_loss: -0.9979\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9980 - val_loss: -0.9983\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9984 - val_loss: -0.9985\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9985 - val_loss: -0.9987\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 1s 11ms/step - loss: -0.9988 - val_loss: -0.9991\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9990 - val_loss: -0.9991\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 1s 11ms/step - loss: -0.9991 - val_loss: -0.9992\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9992 - val_loss: -0.9993\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9994 - val_loss: -0.9994\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9995 - val_loss: -0.9995\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9995 - val_loss: -0.9996\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9995 - val_loss: -0.9996\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9996 - val_loss: -0.9996\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9996 - val_loss: -0.9996\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9996 - val_loss: -0.9997\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: -0.9997 - val_loss: -0.9998\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9997 - val_loss: -0.9997\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9997 - val_loss: -0.9998\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9997\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9998 - val_loss: -0.9998\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9998 - val_loss: -0.9999\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9998\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 1s 16ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -0.9999 - val_loss: -1.0000\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -0.9999\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -0.9999\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -0.9999 - val_loss: -1.0000\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 1s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 1s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 0s 11ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 0s 10ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 0s 6ms/step - loss: -1.0000 - val_loss: -1.0000\n",
      "\n",
      "Train/Test model DeepForest on Fold #0.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.896551724137931\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.9017241379310345\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.903448275862069\n",
      "Adding/Training Layer, n_layer=5\n",
      "Layer validation accuracy = 0.9017241379310345\n",
      "\n",
      "Train/Test model DeepForest on Fold #1.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8896551724137931\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8948275862068965\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8913793103448275\n",
      "\n",
      "Train/Test model DeepForest on Fold #2.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9017241379310345\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.9017241379310345\n",
      "\n",
      "Train/Test model DeepForest on Fold #3.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9189655172413793\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9224137931034483\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.9189655172413793\n",
      "\n",
      "Train/Test model DeepForest on Fold #4.\n",
      "Training MGS Random Forests...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9017241379310345\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9017241379310345\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "error_list = []\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Encoding_Type\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_setting_path):\n",
    "    for file in files:\n",
    "        if (os.path.splitext(file)[-1] == datafile_extensions) & (file.split(\".\")[0].split(\"_\")[-1] == \"ALL-v2\"):\n",
    "            \n",
    "#             try:\n",
    "            \n",
    "            current_dataset_variety = root.split(\"\\\\\")[len(root.split(\"\\\\\"))-1]\n",
    "            encoding_type = file.split(\".\")[0].split(\"_\")[-1]\n",
    "\n",
    "            ##################################################################################\n",
    "            ##### read the current file\n",
    "            ##################################################################################\n",
    "\n",
    "            input_file_full_path = os.path.join(root, file)\n",
    "            sequences_df = pd.read_csv(input_file_full_path, header = \"infer\", low_memory=False)\n",
    "\n",
    "            ##################################################################################\n",
    "            ##### extract data from the current dataframe file\n",
    "            ##################################################################################\n",
    "\n",
    "            sequences_df[\"class\"] = np.where(sequences_df[sequences_df.columns[0]].str.contains(\"nucleosomal\"), 1, 0)\n",
    "\n",
    "            print(\"\\n======================================================================\")\n",
    "            print(\"\\nFile: \"+os.path.join(root, file))\n",
    "            print(\"Nucleosomi: \"+str(sum(sequences_df[\"class\"])))\n",
    "            print(\"Linker: \"+str(len(sequences_df) - sum(sequences_df[\"class\"])))\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##### Extract Feature/Label vectors\n",
    "            ##################################################################################\n",
    "            \n",
    "            ## create the features and labels datasets for the training\n",
    "            labels = np.array(sequences_df[\"class\"])\n",
    "            features = sequences_df.drop(\"nameseq\", axis = 1).drop(\"class\", axis = 1).values\n",
    "            # features = features.reshape(features.shape + (1,))\n",
    "            \n",
    "            x_train, x_valid, y_train, y_valid = train_test_split(features, labels, test_size=0.2)\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##### Encoding the MathFeature vectors using a simple AutoEncoder model\n",
    "            ##################################################################################\n",
    "            \n",
    "            ## Create and set directory to save model\n",
    "            ae_modelPath = os.path.join(result_output_path, current_dataset_variety, \n",
    "                                        \"{}fold\".format(n_fold), \n",
    "                                        \"models\", \"AutoEncoder\")\n",
    "            if(not os.path.isdir(ae_modelPath)):\n",
    "                os.makedirs(ae_modelPath)\n",
    "                \n",
    "            ## Generate model object\n",
    "            ae_model, encoder = autoEncoder_1D(input_length = features.shape[1], \n",
    "                                               learn_rate = 0.1, \n",
    "                                               loss = \"cosine_similarity\")\n",
    "            ## Define model training callbacks\n",
    "#             modelCallbacks = [\n",
    "#                 tf.keras.callbacks.ModelCheckpoint(os.path.join(ae_modelPath, \"{}_bestModel.hdf5\".format(\"AutoEncoder\")),\n",
    "#                                                    monitor = 'val_loss', verbose = 0, save_best_only = True, \n",
    "#                                                    save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "#                 tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 0, \n",
    "#                                                  mode = 'auto', baseline = None, restore_best_weights = False)\n",
    "#             ]\n",
    "            modelCallbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint(os.path.join(ae_modelPath, \"{}_bestModel.hdf5\".format(\"AutoEncoder\")),\n",
    "                                                   monitor = 'val_loss', verbose = 0, save_best_only = True, \n",
    "                                                   save_weights_only = False, mode = 'auto', save_freq = 'epoch')\n",
    "            ]\n",
    "            ## Train the AutoEncoder\n",
    "            ae_model.fit(x = x_train, \n",
    "                         y = x_train, \n",
    "                         batch_size = batch_size, epochs = epochs, verbose = 1, callbacks = modelCallbacks, \n",
    "                         validation_data = (x_valid, x_valid))\n",
    "            \n",
    "            ## Generate Latent-Space representation for the entire feature set\n",
    "            features_encoded = encoder.predict(features)\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##### Generate Folds from dataset, and store to file\n",
    "            ##################################################################################\n",
    "            \n",
    "            ## Parameters to Read/Write the k-fold dataset to file\n",
    "            foldPath = os.path.join(result_output_path, current_dataset_variety, \"{}fold\".format(n_fold))\n",
    "            foldName = file.split(\".\")[0]+\"_{}fold\".format(n_fold)+\".pickle\"\n",
    "\n",
    "            ##### ADDITIONAL CHANGES - USE PREVIOUS GENERATED FOLDS IF AVAILABLE\n",
    "\n",
    "            if(os.path.isfile(os.path.join(foldPath, foldName))):\n",
    "                folds = pickle.load(open(os.path.join(foldPath, foldName), \"rb\"))\n",
    "            else:\n",
    "                ## Generate the k-fold dataset\n",
    "                # folds = build_kfold(features, labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "                folds = build_kfold(features_encoded, labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "                if(not os.path.isdir(foldPath)):\n",
    "                    os.makedirs(foldPath)\n",
    "                pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))\n",
    "            \n",
    "            ##################################################################################\n",
    "            ##### Run Models for all folds\n",
    "            ##################################################################################\n",
    "            \n",
    "            for modelName in modelNames:\n",
    "\n",
    "                ## Create and set directory to save model\n",
    "                modelPath = os.path.join(result_output_path, current_dataset_variety, \"{}fold\".format(n_fold), \"models\", modelName)\n",
    "                if(not os.path.isdir(modelPath)):\n",
    "                    os.makedirs(modelPath)\n",
    "\n",
    "                ## fold counter\n",
    "                i = 0\n",
    "\n",
    "                for fold in folds:\n",
    "\n",
    "                    print(\"\\nTrain/Test model \"+modelName+\" on Fold #\"+str(i)+\".\")\n",
    "                    \n",
    "                    model = DForest(shape_1X=fold[\"X_train\"].shape[1], n_mgsRFtree=200, window=[20, 40], stride=2,\n",
    "                                    cascade_test_size=0.2, n_cascadeRF=2, n_cascadeRFtree=200, cascade_layer=np.inf,\n",
    "                                    min_samples_mgs=0.00001, min_samples_cascade=0.0005, tolerance=0.0, n_jobs=-1)\n",
    "                    \n",
    "                    model.fit(X = fold[\"X_train\"], \n",
    "                              y = fold[\"y_train\"])\n",
    "                    \n",
    "                    model_filename = \"{}_fold{}_model.pickle\".format(modelName, i)\n",
    "                    model_file_obj = open(os.path.join(modelPath, model_filename), 'wb')\n",
    "                    pickle.dump(model, model_file_obj)\n",
    "                    model_file_obj.close()\n",
    "                    \n",
    "                    ##################################################################################\n",
    "                    ##### Prediction and metrics for TRAIN dataset\n",
    "                    ##################################################################################\n",
    "\n",
    "                    y_pred = model.predict(fold[\"X_train\"])\n",
    "                    label_pred = pred2label(y_pred)\n",
    "                    \n",
    "                    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "                    acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "                    prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "\n",
    "                    conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "                    if(conf[0][0]+conf[1][0]):\n",
    "                        sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "                    else:\n",
    "                        sens = 0.0\n",
    "                    if(conf[1][1]+conf[0][1]):\n",
    "                        spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "                    else:\n",
    "                        spec = 0.0\n",
    "                    if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                        mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "                    else:\n",
    "                        mcc= 0.0\n",
    "                    fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "                    auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "\n",
    "                    evaluations[\"Model\"].append(modelName)\n",
    "                    evaluations[\"Encoding_Type\"].append(encoding_type)\n",
    "                    evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "                    evaluations[\"Fold\"].append(i)\n",
    "                    evaluations[\"Train_Test\"].append(\"Train\")\n",
    "                    evaluations[\"Accuracy\"].append(acc)\n",
    "                    evaluations[\"Precision\"].append(prec)\n",
    "                    evaluations[\"TPR\"].append(tpr)\n",
    "                    evaluations[\"FPR\"].append(fpr)\n",
    "                    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "                    evaluations[\"AUC\"].append(auc)\n",
    "                    evaluations[\"Sensitivity\"].append(sens)\n",
    "                    evaluations[\"Specificity\"].append(spec)\n",
    "                    evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "                    ##################################################################################\n",
    "                    ##### Prediction and metrics for TEST dataset\n",
    "                    ##################################################################################\n",
    "\n",
    "                    y_pred = model.predict(fold[\"X_test\"])\n",
    "                    label_pred = pred2label(y_pred)\n",
    "                    \n",
    "                    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "                    acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "                    prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "\n",
    "                    conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "                    if(conf[0][0]+conf[1][0]):\n",
    "                        sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "                    else:\n",
    "                        sens = 0.0\n",
    "                    if(conf[1][1]+conf[0][1]):\n",
    "                        spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "                    else:\n",
    "                        spec = 0.0\n",
    "                    if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "                        mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "                    else:\n",
    "                        mcc= 0.0\n",
    "                    fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "                    auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "\n",
    "                    evaluations[\"Model\"].append(modelName)\n",
    "                    evaluations[\"Encoding_Type\"].append(encoding_type)\n",
    "                    evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "                    evaluations[\"Fold\"].append(i)\n",
    "                    evaluations[\"Train_Test\"].append(\"Test\")\n",
    "                    evaluations[\"Accuracy\"].append(acc)\n",
    "                    evaluations[\"Precision\"].append(prec)\n",
    "                    evaluations[\"TPR\"].append(tpr)\n",
    "                    evaluations[\"FPR\"].append(fpr)\n",
    "                    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "                    evaluations[\"AUC\"].append(auc)\n",
    "                    evaluations[\"Sensitivity\"].append(sens)\n",
    "                    evaluations[\"Specificity\"].append(spec)\n",
    "                    evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "                    i = i+1\n",
    "                        \n",
    "#             except Exception as error:\n",
    "#                 error_list.append((input_file_full_path, error))\n",
    "                \n",
    "##################################################################################\n",
    "##### Dump evaluations to a file\n",
    "##################################################################################\n",
    "\n",
    "evalPath = os.path.join(result_output_path, \"_Evaluation_All_Datasets\", \"{}fold\".format(n_fold))\n",
    "if(not os.path.isdir(evalPath)):\n",
    "    os.makedirs(evalPath)\n",
    "\n",
    "pickle.dump(evaluations,\n",
    "            open(os.path.join(evalPath, \"{}fold_evaluations_{}.pickle\".format(n_fold, modelNames[0])), \"wb\"))\n",
    "\n",
    "##################################################################################\n",
    "##### Dump exceptions to a file\n",
    "##################################################################################\n",
    "\n",
    "pickle.dump(error_list,\n",
    "            open(os.path.join(result_output_path, \"exceptions.pickle\"), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Add import statement here, to make this next part of code standalone executable\n",
    "##################################################################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Parameters used only in this section\n",
    "# ##################################################################################\n",
    "\n",
    "# n_fold = 10\n",
    "\n",
    "# expName = \"MathFeature_setting1_kgap_fickett\"\n",
    "# outPath = \"Generated\"\n",
    "# setting = \"Setting1\"\n",
    "# output_path = \"Results\"\n",
    "\n",
    "# modelNames = [\"RandomForest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Load file and convert to dataframe for easy manipulation\n",
    "##################################################################################\n",
    "\n",
    "evalPath = os.path.join(outPath, expName, output_path, setting, \"_Evaluation_All_Datasets\", \"{}fold\".format(n_fold))\n",
    "\n",
    "evaluations = pickle.load(open(os.path.join(evalPath, \"{}fold_evaluations_{}.pickle\".format(n_fold, modelNames[0])), \"rb\"))\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Encoding_Type</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.793043</td>\n",
       "      <td>0.841317</td>\n",
       "      <td>[0.0, 0.7267241379310345, 1.0]</td>\n",
       "      <td>[0.0, 0.1394736842105263, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.793625</td>\n",
       "      <td>0.755778</td>\n",
       "      <td>0.841317</td>\n",
       "      <td>0.592153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.632174</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>[0.0, 0.5413793103448276, 1.0]</td>\n",
       "      <td>[0.0, 0.2754385964912281, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.632970</td>\n",
       "      <td>0.608247</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.270390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.758043</td>\n",
       "      <td>0.820840</td>\n",
       "      <td>[0.0, 0.6655172413793103, 1.0]</td>\n",
       "      <td>[0.0, 0.14780701754385964, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.758855</td>\n",
       "      <td>0.714601</td>\n",
       "      <td>0.820840</td>\n",
       "      <td>0.526501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.654783</td>\n",
       "      <td>0.700219</td>\n",
       "      <td>[0.0, 0.5517241379310345, 1.0]</td>\n",
       "      <td>[0.0, 0.24035087719298245, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.655687</td>\n",
       "      <td>0.624820</td>\n",
       "      <td>0.700219</td>\n",
       "      <td>0.318132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.761304</td>\n",
       "      <td>0.787124</td>\n",
       "      <td>[0.0, 0.7219827586206896, 1.0]</td>\n",
       "      <td>[0.0, 0.1986842105263158, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.761649</td>\n",
       "      <td>0.739078</td>\n",
       "      <td>0.787124</td>\n",
       "      <td>0.524748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.648696</td>\n",
       "      <td>0.670543</td>\n",
       "      <td>[0.0, 0.596551724137931, 1.0]</td>\n",
       "      <td>[0.0, 0.2982456140350877, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.649153</td>\n",
       "      <td>0.630915</td>\n",
       "      <td>0.670543</td>\n",
       "      <td>0.299878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.742174</td>\n",
       "      <td>0.801917</td>\n",
       "      <td>[0.0, 0.6491379310344828, 1.0]</td>\n",
       "      <td>[0.0, 0.1631578947368421, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.742990</td>\n",
       "      <td>0.700955</td>\n",
       "      <td>0.801917</td>\n",
       "      <td>0.494354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.632174</td>\n",
       "      <td>0.667377</td>\n",
       "      <td>[0.0, 0.5396551724137931, 1.0]</td>\n",
       "      <td>[0.0, 0.2736842105263158, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.632985</td>\n",
       "      <td>0.607930</td>\n",
       "      <td>0.667377</td>\n",
       "      <td>0.270599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>4</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.786522</td>\n",
       "      <td>0.817966</td>\n",
       "      <td>[0.0, 0.7418103448275862, 1.0]</td>\n",
       "      <td>[0.0, 0.16798245614035087, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.786914</td>\n",
       "      <td>0.760016</td>\n",
       "      <td>0.817966</td>\n",
       "      <td>0.575901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Drosophila</td>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.640870</td>\n",
       "      <td>0.649910</td>\n",
       "      <td>[0.0, 0.6241379310344828, 1.0]</td>\n",
       "      <td>[0.0, 0.34210526315789475, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.641016</td>\n",
       "      <td>0.632378</td>\n",
       "      <td>0.649910</td>\n",
       "      <td>0.282160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.867633</td>\n",
       "      <td>0.813672</td>\n",
       "      <td>[0.0, 0.950803701899659, 1.0]</td>\n",
       "      <td>[0.0, 0.21418303785337806, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.868310</td>\n",
       "      <td>0.941987</td>\n",
       "      <td>0.813672</td>\n",
       "      <td>0.746079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.771981</td>\n",
       "      <td>0.730897</td>\n",
       "      <td>[0.0, 0.8560311284046692, 1.0]</td>\n",
       "      <td>[0.0, 0.31094049904030713, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.772545</td>\n",
       "      <td>0.829099</td>\n",
       "      <td>0.730897</td>\n",
       "      <td>0.552493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.864976</td>\n",
       "      <td>0.813076</td>\n",
       "      <td>[0.0, 0.9449585971748661, 1.0]</td>\n",
       "      <td>[0.0, 0.2137038811691423, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.865627</td>\n",
       "      <td>0.935576</td>\n",
       "      <td>0.813076</td>\n",
       "      <td>0.739902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.770048</td>\n",
       "      <td>0.731544</td>\n",
       "      <td>[0.0, 0.8482490272373541, 1.0]</td>\n",
       "      <td>[0.0, 0.30710172744721687, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.770574</td>\n",
       "      <td>0.822323</td>\n",
       "      <td>0.731544</td>\n",
       "      <td>0.547470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.857005</td>\n",
       "      <td>0.796431</td>\n",
       "      <td>[0.0, 0.9561830574488802, 1.0]</td>\n",
       "      <td>[0.0, 0.24065196548418025, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.857766</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.796431</td>\n",
       "      <td>0.728973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.788406</td>\n",
       "      <td>0.741776</td>\n",
       "      <td>[0.0, 0.8791423001949318, 1.0]</td>\n",
       "      <td>[0.0, 0.3007662835249042, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.789188</td>\n",
       "      <td>0.854801</td>\n",
       "      <td>0.741776</td>\n",
       "      <td>0.587406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.862560</td>\n",
       "      <td>0.816631</td>\n",
       "      <td>[0.0, 0.9323271665043817, 1.0]</td>\n",
       "      <td>[0.0, 0.20613614573346117, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.863096</td>\n",
       "      <td>0.922563</td>\n",
       "      <td>0.816631</td>\n",
       "      <td>0.732664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.737815</td>\n",
       "      <td>[0.0, 0.8557504873294347, 1.0]</td>\n",
       "      <td>[0.0, 0.2988505747126437, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.778450</td>\n",
       "      <td>0.831818</td>\n",
       "      <td>0.737815</td>\n",
       "      <td>0.563231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>4</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>0.792361</td>\n",
       "      <td>[0.0, 0.9493670886075949, 1.0]</td>\n",
       "      <td>[0.0, 0.24496644295302014, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>0.938058</td>\n",
       "      <td>0.792361</td>\n",
       "      <td>0.717292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Elegans</td>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.789372</td>\n",
       "      <td>0.731554</td>\n",
       "      <td>[0.0, 0.9083820662768031, 1.0]</td>\n",
       "      <td>[0.0, 0.3275862068965517, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.790398</td>\n",
       "      <td>0.881910</td>\n",
       "      <td>0.731554</td>\n",
       "      <td>0.596906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.751230</td>\n",
       "      <td>0.683657</td>\n",
       "      <td>[0.0, 0.9295929592959296, 1.0]</td>\n",
       "      <td>[0.0, 0.425, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.752296</td>\n",
       "      <td>0.892074</td>\n",
       "      <td>0.683657</td>\n",
       "      <td>0.538990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.648087</td>\n",
       "      <td>0.605388</td>\n",
       "      <td>[0.0, 0.8395604395604396, 1.0]</td>\n",
       "      <td>[0.0, 0.5413043478260869, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.649128</td>\n",
       "      <td>0.742958</td>\n",
       "      <td>0.605388</td>\n",
       "      <td>0.322330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.850465</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>[0.0, 0.9548954895489549, 1.0]</td>\n",
       "      <td>[0.0, 0.25271739130434784, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.851089</td>\n",
       "      <td>0.943720</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.717155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.644809</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>[0.0, 0.7428571428571429, 1.0]</td>\n",
       "      <td>[0.0, 0.45217391304347826, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.645342</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.296275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.805905</td>\n",
       "      <td>0.769981</td>\n",
       "      <td>[0.0, 0.8690869086908691, 1.0]</td>\n",
       "      <td>[0.0, 0.2565217391304348, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.806283</td>\n",
       "      <td>0.851806</td>\n",
       "      <td>0.769981</td>\n",
       "      <td>0.617158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.633880</td>\n",
       "      <td>0.622449</td>\n",
       "      <td>[0.0, 0.6703296703296703, 1.0]</td>\n",
       "      <td>[0.0, 0.40217391304347827, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.634078</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.622449</td>\n",
       "      <td>0.268831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.763870</td>\n",
       "      <td>0.727489</td>\n",
       "      <td>[0.0, 0.839472237493128, 1.0]</td>\n",
       "      <td>[0.0, 0.3108695652173913, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.764301</td>\n",
       "      <td>0.812821</td>\n",
       "      <td>0.727489</td>\n",
       "      <td>0.534424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.625821</td>\n",
       "      <td>0.602190</td>\n",
       "      <td>[0.0, 0.7268722466960352, 1.0]</td>\n",
       "      <td>[0.0, 0.47391304347826085, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.626480</td>\n",
       "      <td>0.661202</td>\n",
       "      <td>0.602190</td>\n",
       "      <td>0.258123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>4</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.782454</td>\n",
       "      <td>0.731553</td>\n",
       "      <td>[0.0, 0.8884002199010446, 1.0]</td>\n",
       "      <td>[0.0, 0.3222826086956522, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.783059</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.731553</td>\n",
       "      <td>0.578695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Homo_Sapiens</td>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.611597</td>\n",
       "      <td>0.590826</td>\n",
       "      <td>[0.0, 0.7092511013215859, 1.0]</td>\n",
       "      <td>[0.0, 0.48478260869565215, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>0.642276</td>\n",
       "      <td>0.590826</td>\n",
       "      <td>0.228745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.958564</td>\n",
       "      <td>0.954665</td>\n",
       "      <td>[0.0, 0.9660904255319149, 1.0]</td>\n",
       "      <td>[0.0, 0.04956896551724138, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.958261</td>\n",
       "      <td>0.962882</td>\n",
       "      <td>0.954665</td>\n",
       "      <td>0.917034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.889503</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>[0.0, 0.9122340425531915, 1.0]</td>\n",
       "      <td>[0.0, 0.13505747126436782, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.888588</td>\n",
       "      <td>0.901198</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.778929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.953729</td>\n",
       "      <td>0.949475</td>\n",
       "      <td>[0.0, 0.9621010638297872, 1.0]</td>\n",
       "      <td>[0.0, 0.05531609195402299, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.953392</td>\n",
       "      <td>0.958455</td>\n",
       "      <td>0.949475</td>\n",
       "      <td>0.907357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.908840</td>\n",
       "      <td>0.901554</td>\n",
       "      <td>[0.0, 0.925531914893617, 1.0]</td>\n",
       "      <td>[0.0, 0.10919540229885058, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.908168</td>\n",
       "      <td>0.917160</td>\n",
       "      <td>0.901554</td>\n",
       "      <td>0.817524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.960290</td>\n",
       "      <td>0.960849</td>\n",
       "      <td>[0.0, 0.9627659574468085, 1.0]</td>\n",
       "      <td>[0.0, 0.042385057471264365, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.960190</td>\n",
       "      <td>0.959683</td>\n",
       "      <td>0.960849</td>\n",
       "      <td>0.920457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.900552</td>\n",
       "      <td>0.902116</td>\n",
       "      <td>[0.0, 0.9069148936170213, 1.0]</td>\n",
       "      <td>[0.0, 0.10632183908045977, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.900297</td>\n",
       "      <td>0.898844</td>\n",
       "      <td>0.902116</td>\n",
       "      <td>0.800777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.959945</td>\n",
       "      <td>0.955381</td>\n",
       "      <td>[0.0, 0.9680851063829787, 1.0]</td>\n",
       "      <td>[0.0, 0.04885057471264368, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.959617</td>\n",
       "      <td>0.965015</td>\n",
       "      <td>0.955381</td>\n",
       "      <td>0.919815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.895028</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>[0.0, 0.901595744680851, 1.0]</td>\n",
       "      <td>[0.0, 0.11206896551724138, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.894763</td>\n",
       "      <td>0.893064</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.789708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>4</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.960290</td>\n",
       "      <td>0.962076</td>\n",
       "      <td>[0.0, 0.961436170212766, 1.0]</td>\n",
       "      <td>[0.0, 0.040948275862068964, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.960244</td>\n",
       "      <td>0.958363</td>\n",
       "      <td>0.962076</td>\n",
       "      <td>0.920463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>DeepForest</td>\n",
       "      <td>ALL-v2</td>\n",
       "      <td>Yeast</td>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.910221</td>\n",
       "      <td>0.910290</td>\n",
       "      <td>[0.0, 0.9175531914893617, 1.0]</td>\n",
       "      <td>[0.0, 0.09770114942528736, 1.0]</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "      <td>0.909926</td>\n",
       "      <td>0.910145</td>\n",
       "      <td>0.910290</td>\n",
       "      <td>0.820144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model Encoding_Type       Dataset  Fold Train_Test  Accuracy  \\\n",
       "0   DeepForest        ALL-v2    Drosophila     0      Train  0.793043   \n",
       "1   DeepForest        ALL-v2    Drosophila     0       Test  0.632174   \n",
       "2   DeepForest        ALL-v2    Drosophila     1      Train  0.758043   \n",
       "3   DeepForest        ALL-v2    Drosophila     1       Test  0.654783   \n",
       "4   DeepForest        ALL-v2    Drosophila     2      Train  0.761304   \n",
       "5   DeepForest        ALL-v2    Drosophila     2       Test  0.648696   \n",
       "6   DeepForest        ALL-v2    Drosophila     3      Train  0.742174   \n",
       "7   DeepForest        ALL-v2    Drosophila     3       Test  0.632174   \n",
       "8   DeepForest        ALL-v2    Drosophila     4      Train  0.786522   \n",
       "9   DeepForest        ALL-v2    Drosophila     4       Test  0.640870   \n",
       "10  DeepForest        ALL-v2       Elegans     0      Train  0.867633   \n",
       "11  DeepForest        ALL-v2       Elegans     0       Test  0.771981   \n",
       "12  DeepForest        ALL-v2       Elegans     1      Train  0.864976   \n",
       "13  DeepForest        ALL-v2       Elegans     1       Test  0.770048   \n",
       "14  DeepForest        ALL-v2       Elegans     2      Train  0.857005   \n",
       "15  DeepForest        ALL-v2       Elegans     2       Test  0.788406   \n",
       "16  DeepForest        ALL-v2       Elegans     3      Train  0.862560   \n",
       "17  DeepForest        ALL-v2       Elegans     3       Test  0.777778   \n",
       "18  DeepForest        ALL-v2       Elegans     4      Train  0.851449   \n",
       "19  DeepForest        ALL-v2       Elegans     4       Test  0.789372   \n",
       "20  DeepForest        ALL-v2  Homo_Sapiens     0      Train  0.751230   \n",
       "21  DeepForest        ALL-v2  Homo_Sapiens     0       Test  0.648087   \n",
       "22  DeepForest        ALL-v2  Homo_Sapiens     1      Train  0.850465   \n",
       "23  DeepForest        ALL-v2  Homo_Sapiens     1       Test  0.644809   \n",
       "24  DeepForest        ALL-v2  Homo_Sapiens     2      Train  0.805905   \n",
       "25  DeepForest        ALL-v2  Homo_Sapiens     2       Test  0.633880   \n",
       "26  DeepForest        ALL-v2  Homo_Sapiens     3      Train  0.763870   \n",
       "27  DeepForest        ALL-v2  Homo_Sapiens     3       Test  0.625821   \n",
       "28  DeepForest        ALL-v2  Homo_Sapiens     4      Train  0.782454   \n",
       "29  DeepForest        ALL-v2  Homo_Sapiens     4       Test  0.611597   \n",
       "30  DeepForest        ALL-v2         Yeast     0      Train  0.958564   \n",
       "31  DeepForest        ALL-v2         Yeast     0       Test  0.889503   \n",
       "32  DeepForest        ALL-v2         Yeast     1      Train  0.953729   \n",
       "33  DeepForest        ALL-v2         Yeast     1       Test  0.908840   \n",
       "34  DeepForest        ALL-v2         Yeast     2      Train  0.960290   \n",
       "35  DeepForest        ALL-v2         Yeast     2       Test  0.900552   \n",
       "36  DeepForest        ALL-v2         Yeast     3      Train  0.959945   \n",
       "37  DeepForest        ALL-v2         Yeast     3       Test  0.895028   \n",
       "38  DeepForest        ALL-v2         Yeast     4      Train  0.960290   \n",
       "39  DeepForest        ALL-v2         Yeast     4       Test  0.910221   \n",
       "\n",
       "    Precision                             TPR  \\\n",
       "0    0.841317  [0.0, 0.7267241379310345, 1.0]   \n",
       "1    0.666667  [0.0, 0.5413793103448276, 1.0]   \n",
       "2    0.820840  [0.0, 0.6655172413793103, 1.0]   \n",
       "3    0.700219  [0.0, 0.5517241379310345, 1.0]   \n",
       "4    0.787124  [0.0, 0.7219827586206896, 1.0]   \n",
       "5    0.670543   [0.0, 0.596551724137931, 1.0]   \n",
       "6    0.801917  [0.0, 0.6491379310344828, 1.0]   \n",
       "7    0.667377  [0.0, 0.5396551724137931, 1.0]   \n",
       "8    0.817966  [0.0, 0.7418103448275862, 1.0]   \n",
       "9    0.649910  [0.0, 0.6241379310344828, 1.0]   \n",
       "10   0.813672   [0.0, 0.950803701899659, 1.0]   \n",
       "11   0.730897  [0.0, 0.8560311284046692, 1.0]   \n",
       "12   0.813076  [0.0, 0.9449585971748661, 1.0]   \n",
       "13   0.731544  [0.0, 0.8482490272373541, 1.0]   \n",
       "14   0.796431  [0.0, 0.9561830574488802, 1.0]   \n",
       "15   0.741776  [0.0, 0.8791423001949318, 1.0]   \n",
       "16   0.816631  [0.0, 0.9323271665043817, 1.0]   \n",
       "17   0.737815  [0.0, 0.8557504873294347, 1.0]   \n",
       "18   0.792361  [0.0, 0.9493670886075949, 1.0]   \n",
       "19   0.731554  [0.0, 0.9083820662768031, 1.0]   \n",
       "20   0.683657  [0.0, 0.9295929592959296, 1.0]   \n",
       "21   0.605388  [0.0, 0.8395604395604396, 1.0]   \n",
       "22   0.788732  [0.0, 0.9548954895489549, 1.0]   \n",
       "23   0.619048  [0.0, 0.7428571428571429, 1.0]   \n",
       "24   0.769981  [0.0, 0.8690869086908691, 1.0]   \n",
       "25   0.622449  [0.0, 0.6703296703296703, 1.0]   \n",
       "26   0.727489   [0.0, 0.839472237493128, 1.0]   \n",
       "27   0.602190  [0.0, 0.7268722466960352, 1.0]   \n",
       "28   0.731553  [0.0, 0.8884002199010446, 1.0]   \n",
       "29   0.590826  [0.0, 0.7092511013215859, 1.0]   \n",
       "30   0.954665  [0.0, 0.9660904255319149, 1.0]   \n",
       "31   0.879487  [0.0, 0.9122340425531915, 1.0]   \n",
       "32   0.949475  [0.0, 0.9621010638297872, 1.0]   \n",
       "33   0.901554   [0.0, 0.925531914893617, 1.0]   \n",
       "34   0.960849  [0.0, 0.9627659574468085, 1.0]   \n",
       "35   0.902116  [0.0, 0.9069148936170213, 1.0]   \n",
       "36   0.955381  [0.0, 0.9680851063829787, 1.0]   \n",
       "37   0.896825   [0.0, 0.901595744680851, 1.0]   \n",
       "38   0.962076   [0.0, 0.961436170212766, 1.0]   \n",
       "39   0.910290  [0.0, 0.9175531914893617, 1.0]   \n",
       "\n",
       "                                 FPR TPR_FPR_Thresholds       AUC  \\\n",
       "0     [0.0, 0.1394736842105263, 1.0]          [2, 1, 0]  0.793625   \n",
       "1     [0.0, 0.2754385964912281, 1.0]          [2, 1, 0]  0.632970   \n",
       "2    [0.0, 0.14780701754385964, 1.0]          [2, 1, 0]  0.758855   \n",
       "3    [0.0, 0.24035087719298245, 1.0]          [2, 1, 0]  0.655687   \n",
       "4     [0.0, 0.1986842105263158, 1.0]          [2, 1, 0]  0.761649   \n",
       "5     [0.0, 0.2982456140350877, 1.0]          [2, 1, 0]  0.649153   \n",
       "6     [0.0, 0.1631578947368421, 1.0]          [2, 1, 0]  0.742990   \n",
       "7     [0.0, 0.2736842105263158, 1.0]          [2, 1, 0]  0.632985   \n",
       "8    [0.0, 0.16798245614035087, 1.0]          [2, 1, 0]  0.786914   \n",
       "9    [0.0, 0.34210526315789475, 1.0]          [2, 1, 0]  0.641016   \n",
       "10   [0.0, 0.21418303785337806, 1.0]          [2, 1, 0]  0.868310   \n",
       "11   [0.0, 0.31094049904030713, 1.0]          [2, 1, 0]  0.772545   \n",
       "12    [0.0, 0.2137038811691423, 1.0]          [2, 1, 0]  0.865627   \n",
       "13   [0.0, 0.30710172744721687, 1.0]          [2, 1, 0]  0.770574   \n",
       "14   [0.0, 0.24065196548418025, 1.0]          [2, 1, 0]  0.857766   \n",
       "15    [0.0, 0.3007662835249042, 1.0]          [2, 1, 0]  0.789188   \n",
       "16   [0.0, 0.20613614573346117, 1.0]          [2, 1, 0]  0.863096   \n",
       "17    [0.0, 0.2988505747126437, 1.0]          [2, 1, 0]  0.778450   \n",
       "18   [0.0, 0.24496644295302014, 1.0]          [2, 1, 0]  0.852200   \n",
       "19    [0.0, 0.3275862068965517, 1.0]          [2, 1, 0]  0.790398   \n",
       "20                 [0.0, 0.425, 1.0]          [2, 1, 0]  0.752296   \n",
       "21    [0.0, 0.5413043478260869, 1.0]          [2, 1, 0]  0.649128   \n",
       "22   [0.0, 0.25271739130434784, 1.0]          [2, 1, 0]  0.851089   \n",
       "23   [0.0, 0.45217391304347826, 1.0]          [2, 1, 0]  0.645342   \n",
       "24    [0.0, 0.2565217391304348, 1.0]          [2, 1, 0]  0.806283   \n",
       "25   [0.0, 0.40217391304347827, 1.0]          [2, 1, 0]  0.634078   \n",
       "26    [0.0, 0.3108695652173913, 1.0]          [2, 1, 0]  0.764301   \n",
       "27   [0.0, 0.47391304347826085, 1.0]          [2, 1, 0]  0.626480   \n",
       "28    [0.0, 0.3222826086956522, 1.0]          [2, 1, 0]  0.783059   \n",
       "29   [0.0, 0.48478260869565215, 1.0]          [2, 1, 0]  0.612234   \n",
       "30   [0.0, 0.04956896551724138, 1.0]          [2, 1, 0]  0.958261   \n",
       "31   [0.0, 0.13505747126436782, 1.0]          [2, 1, 0]  0.888588   \n",
       "32   [0.0, 0.05531609195402299, 1.0]          [2, 1, 0]  0.953392   \n",
       "33   [0.0, 0.10919540229885058, 1.0]          [2, 1, 0]  0.908168   \n",
       "34  [0.0, 0.042385057471264365, 1.0]          [2, 1, 0]  0.960190   \n",
       "35   [0.0, 0.10632183908045977, 1.0]          [2, 1, 0]  0.900297   \n",
       "36   [0.0, 0.04885057471264368, 1.0]          [2, 1, 0]  0.959617   \n",
       "37   [0.0, 0.11206896551724138, 1.0]          [2, 1, 0]  0.894763   \n",
       "38  [0.0, 0.040948275862068964, 1.0]          [2, 1, 0]  0.960244   \n",
       "39   [0.0, 0.09770114942528736, 1.0]          [2, 1, 0]  0.909926   \n",
       "\n",
       "    Sensitivity  Specificity       MCC  \n",
       "0      0.755778     0.841317  0.592153  \n",
       "1      0.608247     0.666667  0.270390  \n",
       "2      0.714601     0.820840  0.526501  \n",
       "3      0.624820     0.700219  0.318132  \n",
       "4      0.739078     0.787124  0.524748  \n",
       "5      0.630915     0.670543  0.299878  \n",
       "6      0.700955     0.801917  0.494354  \n",
       "7      0.607930     0.667377  0.270599  \n",
       "8      0.760016     0.817966  0.575901  \n",
       "9      0.632378     0.649910  0.282160  \n",
       "10     0.941987     0.813672  0.746079  \n",
       "11     0.829099     0.730897  0.552493  \n",
       "12     0.935576     0.813076  0.739902  \n",
       "13     0.822323     0.731544  0.547470  \n",
       "14     0.946237     0.796431  0.728973  \n",
       "15     0.854801     0.741776  0.587406  \n",
       "16     0.922563     0.816631  0.732664  \n",
       "17     0.831818     0.737815  0.563231  \n",
       "18     0.938058     0.792361  0.717292  \n",
       "19     0.881910     0.731554  0.596906  \n",
       "20     0.892074     0.683657  0.538990  \n",
       "21     0.742958     0.605388  0.322330  \n",
       "22     0.943720     0.788732  0.717155  \n",
       "23     0.682927     0.619048  0.296275  \n",
       "24     0.851806     0.769981  0.617158  \n",
       "25     0.647059     0.622449  0.268831  \n",
       "26     0.812821     0.727489  0.534424  \n",
       "27     0.661202     0.602190  0.258123  \n",
       "28     0.860000     0.731553  0.578695  \n",
       "29     0.642276     0.590826  0.228745  \n",
       "30     0.962882     0.954665  0.917034  \n",
       "31     0.901198     0.879487  0.778929  \n",
       "32     0.958455     0.949475  0.907357  \n",
       "33     0.917160     0.901554  0.817524  \n",
       "34     0.959683     0.960849  0.920457  \n",
       "35     0.898844     0.902116  0.800777  \n",
       "36     0.965015     0.955381  0.919815  \n",
       "37     0.893064     0.896825  0.789708  \n",
       "38     0.958363     0.962076  0.920463  \n",
       "39     0.910145     0.910290  0.820144  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Group dataset (mean of metrics) by [Dataset, Model, Train_Test] combinations\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Dataset\",\n",
    "                                                 \"Encoding_Type\",\n",
    "                                                 \"Model\", \n",
    "                                                 \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Available :  ['Accuracy', 'Precision', 'AUC', 'Sensitivity', 'Specificity', 'MCC']\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### Decide on metric to visualize\n",
    "##################################################################################\n",
    "\n",
    "print(\"Metrics Available : \", list(evaluations_df_grouped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a metric to plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"Accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### Visualize with a multiple Bar chart\n",
    "##################################################################################\n",
    "\n",
    "# df = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(0), [dataset_to_print])]\n",
    "# df = evaluations_df_grouped.reset_index(level=['Dataset', 'Encoding_Type', 'Train_Test'])\n",
    "df = evaluations_df_grouped.reset_index()\n",
    "\n",
    "# Some boilerplate to initialise things\n",
    "sns.set()\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "# Draw the bars\n",
    "ax = sns.barplot(data=df, x=\"Dataset\", y=metric_to_plot, hue=\"Train_Test\")\n",
    "\n",
    "# Customise some display properties\n",
    "ax.set_title(metric_to_plot)\n",
    "ax.grid(color='#cccccc')\n",
    "ax.set_ylabel(metric_to_plot)\n",
    "ax.set_xlabel(\"Dataset\")\n",
    "ax.set_xticklabels(df[\"Dataset\"].unique().astype(str), rotation='vertical')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height()*100, '.4f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                size=15,\n",
    "                xytext = (0, -12), \n",
    "                textcoords = 'offset points')\n",
    "\n",
    "##############################\n",
    "\n",
    "# Ask Matplotlib to show it\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all metrics' plots to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Iteratively generate comparison plot using every metric\n",
    "##################################################################################\n",
    "\n",
    "for metric_to_plot in list(evaluations_df_grouped.columns):\n",
    "    \n",
    "    # df = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(0), [dataset_to_print])]\n",
    "    # df = evaluations_df_grouped.reset_index(level=['Dataset', 'Encoding_Type', 'Train_Test'])\n",
    "    df = evaluations_df_grouped.reset_index()\n",
    "\n",
    "    # Some boilerplate to initialise things\n",
    "    sns.set()\n",
    "    plt.figure(figsize=(20,8))\n",
    "\n",
    "    # Draw the bars\n",
    "    ax = sns.barplot(data=df, x=\"Dataset\", y=metric_to_plot, hue=\"Train_Test\")\n",
    "\n",
    "    # Customise some display properties\n",
    "    ax.set_title(metric_to_plot+\" for Setting1\")\n",
    "    ax.grid(color='#cccccc')\n",
    "    ax.set_ylabel(metric_to_plot)\n",
    "    ax.set_xlabel(\"Dataset\")\n",
    "    ax.set_xticklabels(df[\"Dataset\"].unique().astype(str), rotation='vertical')\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(format(p.get_height()*100, '.4f'),\n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha = 'center', va = 'center', \n",
    "                    size=15,\n",
    "                    xytext = (0, -12), \n",
    "                    textcoords = 'offset points')\n",
    "\n",
    "    plt.savefig(os.path.join(evalPath, \"{}_{}_Comparison\".format(metric_to_plot, modelNames[0])))\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
