{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import os\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import six.moves.cPickle\n",
    "\n",
    "import r2v_functions_mod as r2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "k = 6\n",
    "\n",
    "dataset_dir_path = \"piRNA\"\n",
    "\n",
    "out_dataset_path = dataset_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = []\n",
    "\n",
    "# for root, dirs, files in os.walk(dataset_dir_path):\n",
    "#     for file in files:\n",
    "#         file_list.append((root, file))\n",
    "\n",
    "file_list = [(dataset_dir_path, 'piRNA_layer1.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing started for file: piRNA\\piRNA_layer1.csv \n",
      "\n",
      "Processing read 0. Last batch: 0.000 minutes. Total time: 0.000 hours.\n",
      "Processing read 1000. Last batch: 0.001 minutes. Total time: 0.000 hours.\n",
      "Processing read 2000. Last batch: 0.001 minutes. Total time: 0.000 hours.\n",
      "Loading total kmers.\n",
      "Loading model.\n",
      "path_sample: piRNA\\piRNA_layer1.fas\n",
      "Total reads in sample piRNA_layer1: 2835.\n",
      "Normalizing each read by total number of kmers in that read.\n",
      "Processing linker_sequence_1: 0/2835.\n",
      "Processing linker_sequence_1001: 1000/2835.\n",
      "Processing nucleosomal_sequence_584: 2000/2835.\n",
      "Saving reads to piRNA\\piRNA_layer1_layer1_6_64_3_5_5_0.0001_10_1e-05_remb_raw.csv.gz.\n",
      "Performing SVD: (64,2835).\n",
      "Saving reads to piRNA\\piRNA_layer1_layer1_6_64_3_5_5_0.0001_10_1e-05_remb.csv.gz.\n",
      "\n",
      "Processing completed for current file.\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "for current_file in file_list:\n",
    "    \n",
    "    print(\"\\nProcessing started for file:\", os.path.join(current_file[0], current_file[1]), \"\\n\")\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### getting file in/out paths and file names ready\n",
    "    ##################################################################################\n",
    "    \n",
    "    in_current_file_name = current_file[1]\n",
    "    \n",
    "    in_current_file_path = os.path.join(current_file[0],\n",
    "                                        current_file[1])\n",
    "    \n",
    "    out_current_kmers_file_name = current_file[1][0:current_file[1].find(\".\")]+\"_{}mers\".format(k)+\".fas\"\n",
    "    \n",
    "    out_current_file_name = current_file[1][0:current_file[1].find(\".\")]+\"_word2vec_embedded\"+current_file[1][current_file[1].find(\".\"):len(current_file[1])]\n",
    "    \n",
    "    out_current_dir_path = out_dataset_path\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### read CSV file\n",
    "    ##################################################################################\n",
    "    \n",
    "    org_ACGU_data_list = []\n",
    "    max_len = 0\n",
    "    \n",
    "    with open(in_current_file_path, newline='\\n') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in spamreader:\n",
    "            if len(row[0]) > max_len:\n",
    "                max_len = len(row[0])\n",
    "            org_ACGU_data_list.append(row)\n",
    "\n",
    "    ##################################################################################\n",
    "    ##### pre process data as required by the models\n",
    "    ##################################################################################\n",
    "    \n",
    "    org_ACGT_data_list = []\n",
    "\n",
    "    for row in org_ACGU_data_list: \n",
    "        org_ACGT_data_list.append([row[0].replace(\"U\", \"T\"), int(row[1])])\n",
    "    \n",
    "    id_List = [row[1] for row in org_ACGT_data_list]\n",
    "    seq_List = [row[0] for row in org_ACGT_data_list]\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Write SEQ information to a fasta sequence file for word2vec processing\n",
    "    ##################################################################################\n",
    "    \n",
    "    def generator_of_sequences(seq_list, id_list):\n",
    "        id_n = 0\n",
    "        id_l = 0\n",
    "        for string_seq, id_01 in zip(seq_list, id_list):\n",
    "            if id_01 == 0:\n",
    "                id_l = id_l + 1\n",
    "                push_id = \"linker_sequence_{}\".format(id_l)\n",
    "            elif id_01 == 1:\n",
    "                id_n = id_n + 1\n",
    "                push_id = \"nucleosomal_sequence_{}\".format(id_n)\n",
    "            yield SeqRecord(Seq(string_seq), id=push_id)\n",
    "    \n",
    "    path_reads = os.path.join(out_current_dir_path, in_current_file_name.split(\".\")[0]+\".fas\")\n",
    "    output_handle = open(path_reads, \"w\")\n",
    "    SeqIO.write(generator_of_sequences(seq_List, id_List), output_handle, \"fasta\")\n",
    "    output_handle.close()\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Generate k-mers and write to file\n",
    "    ##################################################################################\n",
    "    \n",
    "    if(not os.path.isdir(out_current_dir_path)):\n",
    "        os.makedirs(out_current_dir_path)\n",
    "        \n",
    "    kmers_path = os.path.join(out_current_dir_path, out_current_kmers_file_name+\".gz\")\n",
    "    \n",
    "    out_kmers = gzip.open(kmers_path,'w')\n",
    "    \n",
    "    for sequence in seq_List:\n",
    "        curr_seq_kmers = []\n",
    "        for i in range(0,len(seq_List[0]) - k + 1):\n",
    "            curr_seq_kmers.append(sequence[i:i+k])\n",
    "\n",
    "        curr_seq_kmers_joined = \" \".join(map(str, curr_seq_kmers))+\"\\n\"\n",
    "        out_kmers.write(curr_seq_kmers_joined.encode())\n",
    "\n",
    "    out_kmers.close()\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### word2vec Model Training parameters\n",
    "    ##################################################################################\n",
    "\n",
    "    seed = random.randint(1,9999999)\n",
    "    d = 64\n",
    "    w = 5\n",
    "    neg_samps = 5\n",
    "    samp_freq = 0.0001\n",
    "    n_min = 10\n",
    "    epochs = 3\n",
    "    n_cores = 1\n",
    "    prefix = in_current_file_name[0:in_current_file_name.rfind(\".\")]\n",
    "    \n",
    "    model_fn = prefix + '_' + str(k) + '_' + str(d) + \\\n",
    "        '_' + str(epochs) + '_' + str(w) + '_' + \\\n",
    "        str(neg_samps).replace('0.','') + '_' + \\\n",
    "        str(samp_freq) + '_' + str(n_min) + '_model.pkl'\n",
    "\n",
    "    model_file_path = os.path.join(out_current_dir_path, model_fn)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Train word2vec Model\n",
    "    ##################################################################################\n",
    "    \n",
    "    kmers_init = LineSentence(kmers_path)\n",
    "    \n",
    "    model = Word2Vec(kmers_init, sg=1, size=d, window=w, min_count=n_min, negative=neg_samps,\n",
    "                     sample=samp_freq, iter=epochs, workers=n_cores, seed=seed)\n",
    "\n",
    "    model.save(model_file_path)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Embedding Parameters \n",
    "    ##################################################################################\n",
    "\n",
    "    nr = bool(int(1))\n",
    "    a = 1e-05\n",
    "    v = 1000\n",
    "\n",
    "    #path_reads = in_current_file_path\n",
    "    path_model = model_file_path\n",
    "    \n",
    "    fn_totalkmers = '%s_%s_totalkmers.pkl' % (prefix,str(k))\n",
    "\n",
    "    path_totalkmers = os.path.join(out_current_dir_path, fn_totalkmers)\n",
    "\n",
    "    work_dir = out_current_dir_path\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Generating the read embeddings from the sequences using word2vec\n",
    "    ##################################################################################\n",
    "    \n",
    "    total_kmers = r2v.calc_total_kmers(path_reads, path_model, k, verbose=True, v=v)\n",
    "    \n",
    "    six.moves.cPickle.dump(total_kmers, open(path_totalkmers, 'wb'), protocol=4)\n",
    "    \n",
    "    r2v.embed_reads(path_sample = path_reads, path_totalkmers = path_totalkmers, path_model = path_model, path_out = work_dir, normread=nr, k=k, a=a, verbose=True, v=v)\n",
    "    \n",
    "    print(\"\\nProcessing completed for current file.\\n============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'piRNA\\\\piRNA_layer1.fas'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'piRNA\\\\piRNA_layer1_6_64_3_5_5_0.0001_10_model.pkl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
