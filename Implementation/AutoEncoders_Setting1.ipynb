{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 10\n",
    "expName = \"bi_LSTM_AE_Setting1_v2.0\"\n",
    "outPath = \"Generated\"\n",
    "foldName = \"folds.pickle\"\n",
    "\n",
    "modelNames = [\"Bi_LSTM_AE\"]\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "shuffle = False\n",
    "seed = 123\n",
    "\n",
    "dataset_path = \"CORENup-Datasets\\\\Datasets\"\n",
    "setting = \"Setting1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Checking the directory\n",
    "##################################################################################\n",
    "\n",
    "dataset_setting_path = os.path.join(dataset_path, setting)\n",
    "dataset_varieties = next(os.walk(dataset_setting_path))\n",
    "print(dataset_varieties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define custom one hot encoding, contains code from repository\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    \"\"\"\n",
    "    COMMENT\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),4))\n",
    "    dict_nuc = {\n",
    "        \"A\": 0,\n",
    "        \"C\": 1,\n",
    "        \"G\": 2,\n",
    "        \"T\":3\n",
    "    }\n",
    "    i = 0\n",
    "    \n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in dict_nuc.keys()):\n",
    "            seq_encoded[i][dict_nuc[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    return seq_encoded\n",
    "\n",
    "##################################################################################\n",
    "##### define K-fold building functions\n",
    "##################################################################################\n",
    "\n",
    "def build_kfold(features, \n",
    "                labels, k=10, \n",
    "                shuffle=False, \n",
    "                seed=None):\n",
    "    \"\"\"\n",
    "    COMMENT\n",
    "    \"\"\"\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n",
    "##################################################################################\n",
    "##### prediction encoding functions\n",
    "##################################################################################\n",
    "\n",
    "def row_convert(row):\n",
    "    \"\"\"\n",
    "    COMMENT\n",
    "    \"\"\"\n",
    "    return (row == row.max(axis=1)[:,None]).astype(float)\n",
    "\n",
    "def pred2label_AE(y_pred):\n",
    "    \"\"\"\n",
    "    COMMENT\n",
    "    \"\"\"\n",
    "    return np.array([row_convert(row) for row in y_pred])\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    \"\"\"\n",
    "    COMMENT\n",
    "    \"\"\"\n",
    "    y_pred = np.round(np.clip(y_pred, 0, 1))\n",
    "    return y_pred\n",
    "\n",
    "##################################################################################\n",
    "##### custom reconstruction accuracy function\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_decode_integer(sequence):\n",
    "    \"\"\"\n",
    "    COMMENT\n",
    "    \"\"\"\n",
    "    return np.argmax(sequence, axis=1)\n",
    "\n",
    "def average_reconstruction_accuracy(y_true, \n",
    "                                    y_pred_label):\n",
    "    \"\"\"\n",
    "    COMMENT\n",
    "    \"\"\"\n",
    "    if len(y_true) == len(y_pred_label):\n",
    "        acc_scores = [accuracy_score(one_hot_decode_integer(pair[0]), one_hot_decode_integer(pair[1])) for pair in zip(y_true, y_pred_label)]\n",
    "        return np.mean(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_AutoEncoder(input_shape = (150,4),\n",
    "                       latent_space_size = 64,\n",
    "                       learn_rate = 0.001, \n",
    "                       loss = 'categorical_crossentropy', \n",
    "                       metrics = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Simple Autoencoder, full connected network.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # encoded = Dense(units = latent_space_size*2, activation='relu')(inputs)\n",
    "    \n",
    "    encoder = tf.keras.layers.Dense(units = latent_space_size)(inputs)\n",
    "    \n",
    "    decoder = tf.keras.layers.Dense(units=input_shape[1], activation='sigmoid')(encoder)\n",
    "    \n",
    "    autoencoder = tf.keras.Model(inputs, decoder)\n",
    "    encoder = tf.keras.Model(inputs, encoder)\n",
    "    \n",
    "    autoencoder.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss)\n",
    "    \n",
    "#     return autoencoder, encoder\n",
    "\n",
    "# def test_encoder(input_shape = (150,4),\n",
    "#                  latent_space_size = 64,\n",
    "#                  learn_rate = 0.001, \n",
    "#                  loss = 'categorical_crossentropy', \n",
    "#                  metrics = None):\n",
    "    \n",
    "#     inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "#     # Encoder layers\n",
    "#     encoder = tf.keras.layers.LSTM(units = latent_space_size*4, return_sequences=True, activation='relu')(inputs)\n",
    "#     encoder = tf.keras.layers.LSTM(units = latent_space_size*2, return_sequences=True, activation='relu')(encoder)\n",
    "#     encoder = tf.keras.layers.LSTM(units = int(latent_space_size*1.5), return_sequences=True, activation='relu')(encoder)\n",
    "#     encoder = tf.keras.layers.LSTM(units = latent_space_size, return_sequences=False, activation='relu')(encoder)\n",
    "    \n",
    "#     # Decoder Layers\n",
    "#     dec_input = tf.keras.layers.RepeatVector(input_shape[0])(encoder)\n",
    "#     decoder = tf.keras.layers.LSTM(units = int(latent_space_size*1.5), return_sequences=True, activation='relu')(dec_input)\n",
    "#     decoder = tf.keras.layers.LSTM(units = latent_space_size*2, return_sequences=True, activation='relu')(decoder)\n",
    "#     decoder = tf.keras.layers.LSTM(units = latent_space_size*4, return_sequences=True, activation='relu')(decoder)\n",
    "#     decoder = tf.keras.layers.LSTM(units = input_shape[1], return_sequences=True, activation='sigmoid')(decoder)\n",
    "#     # decoder = tf.keras.layers.LSTM(units = input_shape[0], return_sequences=True, activation='tanh')(dec_input)\n",
    "    \n",
    "#     # Building the model\n",
    "#     autoencoder = tf.keras.Model(inputs, decoder)\n",
    "#     encoder = tf.keras.Model(inputs, encoder)\n",
    "    \n",
    "#     # Compiling the model to add hyper-parameters\n",
    "#     autoencoder.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss)\n",
    "    \n",
    "#     return autoencoder, encoder\n",
    "\n",
    "def test_encoder(input_shape = (150,4),\n",
    "                 latent_space_size = 128,\n",
    "                 learn_rate = 0.001, \n",
    "                 loss = 'categorical_crossentropy', \n",
    "                 metrics = None):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder layers\n",
    "    encoder = tf.keras.layers.LSTM(units = latent_space_size*4, return_sequences=True)(inputs)\n",
    "    encoder = tf.keras.layers.LSTM(units = latent_space_size*2, return_sequences=True)(encoder)\n",
    "    encoder = tf.keras.layers.LSTM(units = int(latent_space_size*1.5), return_sequences=True)(encoder)\n",
    "    encoder = tf.keras.layers.LSTM(units = latent_space_size, return_sequences=False)(encoder)\n",
    "    \n",
    "    # Decoder Layers\n",
    "    dec_input = tf.keras.layers.RepeatVector(input_shape[0])(encoder)\n",
    "    decoder = tf.keras.layers.LSTM(units = int(latent_space_size*1.5), return_sequences=True)(dec_input)\n",
    "    decoder = tf.keras.layers.LSTM(units = latent_space_size*2, return_sequences=True)(decoder)\n",
    "    decoder = tf.keras.layers.LSTM(units = latent_space_size*4, return_sequences=True)(decoder)\n",
    "    decoder = tf.keras.layers.LSTM(units = input_shape[1], return_sequences=True, activation='softmax')(decoder)\n",
    "    \n",
    "    # Building the model\n",
    "    autoencoder = tf.keras.Model(inputs, decoder)\n",
    "    encoder = tf.keras.Model(inputs, encoder)\n",
    "    \n",
    "    # Compiling the model to add hyper-parameters\n",
    "    autoencoder.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss)\n",
    "    \n",
    "    return autoencoder, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model, encoder = test_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Functions to customize the LSTM AE network\n",
    "##################################################################################\n",
    "\n",
    "def biLSTM_AutoEncoder(input_shape = (150,4),\n",
    "                       lstm_decode_units = 100, ## LSTM layer 1 parameters\n",
    "                       latent_space_size = 128,\n",
    "                       learn_rate = 0.001, loss = 'categorical_crossentropy', metrics = None):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    encoded = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = lstm_decode_units,\n",
    "                                                                 activation='softmax', \n",
    "                                                                 return_sequences = True))(inputs)\n",
    "    \n",
    "    encoded = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = lstm_decode_units,\n",
    "                                                                 activation='softmax', \n",
    "                                                                 return_sequences = False))(encoded)\n",
    "    \n",
    "    # encoded = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=1))(encoded)\n",
    "    encoded = tf.keras.layers.Dense(units = latent_space_size)(encoded)\n",
    "    \n",
    "    decoded = tf.keras.layers.RepeatVector(input_shape[0])(encoded)\n",
    "    decoded = tf.keras.layers.LSTM(units = lstm_decode_units, return_sequences=True)(decoded)\n",
    "    decoded = tf.keras.layers.LSTM(units = input_shape[1], return_sequences=True)(decoded)\n",
    "    decoded = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=input_shape[1], activation='sigmoid'))(decoded)\n",
    "    \n",
    "    sequence_autoencoder = tf.keras.Model(inputs, decoded)\n",
    "    encoder = tf.keras.Model(inputs, encoded)\n",
    "    \n",
    "    sequence_autoencoder.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss)\n",
    "    \n",
    "    return sequence_autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model, encoder = biLSTM_AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Functions to customize a simple fully-connected classification network\n",
    "##################################################################################\n",
    "\n",
    "def biLSTM_Classifier(input_shape=(150,1), \n",
    "                    dense_decode_units = 150,\n",
    "                    learn_rate = 0.01, prob = 0.1, loss = 'binary_crossentropy', metrics = None):\n",
    "    '''\n",
    "    INFO\n",
    "    '''\n",
    "    # beta = 0.001\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape = input_shape))\n",
    "    # model.add(tf.keras.layers.Dense(dense_decode_units, kernel_regularizer=tf.keras.regularizers.l2(beta), activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(dense_decode_units*2, activation='relu', kernel_regularizer=tf.keras.regularizers.l1()))\n",
    "    # model.add(tf.keras.layers.BatchNormalization(trainable = True))\n",
    "    # model.add(tf.keras.layers.Dropout(prob))\n",
    "    # model.add(tf.keras.layers.Dense(dense_decode_units*2, kernel_regularizer=tf.keras.regularizers.l2(beta), activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(dense_decode_units, activation='relu'))\n",
    "    # model.add(tf.keras.layers.BatchNormalization(trainable = True))\n",
    "    # model.add(tf.keras.layers.Dropout(prob))\n",
    "    # model.add(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(beta), activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    #[tf.keras.metrics.binary_accuracy, metrics.precision, metrics.recall, metrics.f1score])\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss, metrics = metrics) \n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Model\" : [],\n",
    "    \"Dataset\" : [],\n",
    "    \"Fold\" : [],\n",
    "    \"AutoEncoder_Categorical_CrossEntropy_Loss\" : [],\n",
    "    \"AutoEncoder_Reconstruction_Accuracy\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "i = 0\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_setting_path):\n",
    "    if i == 0:\n",
    "        i = i+1\n",
    "        continue\n",
    "    for file in files:\n",
    "        \n",
    "        current_dataset_variety = root.split(\"\\\\\")[len(root.split(\"\\\\\"))-1]\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### read the current file\n",
    "        ##################################################################################\n",
    "        \n",
    "        openFile = open(os.path.join(root, file))\n",
    "        fastaSequences = SeqIO.parse(openFile, \"fasta\")\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### extract data from the current fasta file\n",
    "        ##################################################################################\n",
    "        \n",
    "        nucleosomal_List = []\n",
    "        linker_List = []\n",
    "        nucleosomal_onehotencoded_List = []\n",
    "        linker_onehotencoded_List = []\n",
    "        \n",
    "        for fasta in fastaSequences: \n",
    "            name, sequence = fasta.id, str(fasta.seq)\n",
    "            if \"nucleosomal\" in name:\n",
    "                nucleosomal_List.append(sequence)\n",
    "                aus_seq = one_hot_encode(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    nucleosomal_onehotencoded_List.append(aus_seq)\n",
    "            else:\n",
    "                linker_List.append(sequence)\n",
    "                aus_seq = one_hot_encode(sequence)\n",
    "                if(len(aus_seq) != 0):\n",
    "                    linker_onehotencoded_List.append(aus_seq)\n",
    "        \n",
    "        print(\"\\n======================================================================\")\n",
    "        print(\"\\nFile: \"+os.path.join(root, file))\n",
    "        print(\"Nucleosomi: \"+str(len(nucleosomal_onehotencoded_List)))\n",
    "        print(\"Linker: \"+str(len(linker_onehotencoded_List)))\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Generate Folds from dataset, and store to file\n",
    "        ##################################################################################\n",
    "        \n",
    "        ## create the features and labels datasets for the training\n",
    "        input_size = (len(nucleosomal_onehotencoded_List[1]), 4)\n",
    "        labels = np.concatenate((np.ones((len(nucleosomal_onehotencoded_List), 1), dtype=np.float32), np.zeros((len(linker_onehotencoded_List), 1), dtype=np.float32)), axis=0)\n",
    "        features = np.concatenate((nucleosomal_onehotencoded_List,linker_onehotencoded_List), 0)\n",
    "\n",
    "        ## Generate the k-fold dataset\n",
    "        folds = build_kfold(features, labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "        \n",
    "        ## Write the k-fold dataset to file\n",
    "        foldPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold))\n",
    "        \n",
    "        ##### ADDITIONAL CHANGES - USE PREVIOUS GENERATED FOLDS IF AVAILABLE\n",
    "        \n",
    "        if(os.path.isfile(os.path.join(foldPath, foldName))):\n",
    "            folds = pickle.load(open(os.path.join(foldPath, foldName), \"rb\"))\n",
    "        else:\n",
    "            if(not os.path.isdir(foldPath)):\n",
    "                os.makedirs(foldPath)\n",
    "            pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### TRAIN and PREDICT for every Fold, using both models (DLNN3 and DLNN5)\n",
    "        ##################################################################################\n",
    "        \n",
    "        for modelName in modelNames:\n",
    "            \n",
    "            ## Create and set directory to save model\n",
    "            modelPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold), \"models\", modelName)\n",
    "            if(not os.path.isdir(modelPath)):\n",
    "                os.makedirs(modelPath)\n",
    "            \n",
    "            ## fold counter\n",
    "            i = 0\n",
    "\n",
    "            for fold in folds:\n",
    "                \n",
    "                print(\"\\nTrain/Test model \"+modelName+\" on Fold #\"+str(i)+\".\")\n",
    "                \n",
    "                # Available model function input hyper parameters\n",
    "                # input_shape = (150,4),\n",
    "                # lstm_decode_units = 100, ## LSTM layer 1 parameters\n",
    "                # latent_space_size = 128,\n",
    "                # learn_rate = 0.001, loss = 'categorical_crossentropy', metrics = None\n",
    "                \n",
    "                ## Generate AUTOENCODER model using function\n",
    "                ae_model, encoder = biLSTM_AutoEncoder(input_shape = input_size, latent_space_size = input_size[0])\n",
    "                \n",
    "                ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "                modelCallbacks = [\n",
    "                    tf.keras.callbacks.ModelCheckpoint(os.path.join(modelPath, \"{}_{}_bestModel-fold{}.hdf5\".format(modelName, \"AutoEncoder\", i)),\n",
    "                                                       monitor = 'val_loss', verbose = 0, save_best_only = True, \n",
    "                                                       save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 0, \n",
    "                                                     mode = 'auto', baseline = None, restore_best_weights = False)\n",
    "                ]\n",
    "                ae_model.fit(x = fold[\"X_train\"], y = fold[\"X_train\"], batch_size = batch_size, epochs = epochs, verbose = 1,\n",
    "                             callbacks = modelCallbacks, validation_data = (fold[\"X_test\"], fold[\"X_test\"]))\n",
    "                \n",
    "                ## dump Encoder to hdf5\n",
    "                encoder.save(os.path.join(modelPath, \"{}_{}_bestModel-fold{}.hdf5\".format(modelName, \"Encoder\", i)))\n",
    "                \n",
    "                ## Generate embeddings using Encoder model\n",
    "                X_train_encoded = encoder.predict(fold['X_train'])\n",
    "                X_test_encoded = encoder.predict(fold['X_test'])\n",
    "                \n",
    "                X_train_encoded = X_train_encoded.reshape((X_train_encoded.shape[0], X_train_encoded.shape[1]))\n",
    "                X_test_encoded = X_test_encoded.reshape((X_test_encoded.shape[0], X_test_encoded.shape[1]))\n",
    "                \n",
    "#                 ## Generate CLASSIFIER model using function\n",
    "#                 model = FFNN_Classifier(input_shape = (X_train_encoded.shape[1]), dense_decode_units = X_train_encoded.shape[1], learn_rate = 0.005)\n",
    "                \n",
    "#                 ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "#                 modelCallbacks = [\n",
    "#                     tf.keras.callbacks.ModelCheckpoint(os.path.join(modelPath, \"{}_{}_bestModel-fold{}.hdf5\".format(modelName, \"Classifier\", i)),\n",
    "#                                                        monitor = 'val_loss', verbose = 0, save_best_only = True, \n",
    "#                                                        save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "#                     tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 0, \n",
    "#                                                      mode = 'auto', baseline = None, restore_best_weights = False)\n",
    "#                 ]\n",
    "#                 model.fit(x = X_train_encoded, y = fold[\"y_train\"],\n",
    "#                           batch_size = batch_size, epochs = 100, verbose = 0,\n",
    "#                           validation_data = (X_test_encoded, fold[\"y_test\"]),\n",
    "#                           callbacks = modelCallbacks)\n",
    "                \n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = ae_model.predict(fold['X_train'])\n",
    "x_pred_label = pred2label_AE(x_pred)\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce(fold['X_train'], x_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reconstruction_accuracy(fold['X_train'], x_pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold['X_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_pred_label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = ae_model.predict(fold['X_test'])\n",
    "x_pred_label = pred2label_AE(x_pred)\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce(fold['X_test'], x_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reconstruction_accuracy(fold['X_test'], x_pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Functions to customize a simple fully-connected classification network\n",
    "# ##################################################################################\n",
    "\n",
    "# def FFNN_Classifier(input_shape=(150,1), \n",
    "#                     dense_decode_units = 150,\n",
    "#                     learn_rate = 0.01, prob = 0.1, loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "#     # beta = 0.001\n",
    "    \n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(tf.keras.Input(shape = input_shape))\n",
    "#     # model.add(tf.keras.layers.Dense(dense_decode_units, kernel_regularizer=tf.keras.regularizers.l2(beta), activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(dense_decode_units*2, activation='relu'))\n",
    "#     # model.add(tf.keras.layers.BatchNormalization(trainable = True))\n",
    "#     # model.add(tf.keras.layers.Dropout(prob))\n",
    "#     # model.add(tf.keras.layers.Dense(dense_decode_units*2, kernel_regularizer=tf.keras.regularizers.l2(beta), activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(dense_decode_units, activation='relu'))\n",
    "#     # model.add(tf.keras.layers.BatchNormalization(trainable = True))\n",
    "#     # model.add(tf.keras.layers.Dropout(prob))\n",
    "#     # model.add(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(beta), activation='sigmoid'))\n",
    "#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     #[tf.keras.metrics.binary_accuracy, metrics.precision, metrics.recall, metrics.f1score])\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss, metrics = metrics) \n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(lr = learn_rate), loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FFNN_Classifier(input_shape = (X_train_encoded.shape[1]), dense_decode_units = X_train_encoded.shape[1], learn_rate = 0.005)\n",
    "                \n",
    "# ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "# modelCallbacks = [\n",
    "#     tf.keras.callbacks.ModelCheckpoint(os.path.join(modelPath, \"{}_{}_bestModel-fold{}.hdf5\".format(modelName, \"Classifier\", i)),\n",
    "#                                        monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "#                                        save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 0, \n",
    "#                                      mode = 'auto', baseline = None, restore_best_weights = False)\n",
    "# ]\n",
    "# # model.fit(x = X_train_encoded, y = fold[\"y_train\"],\n",
    "# #           batch_size = batch_size, epochs = 100, verbose = 1,\n",
    "# #           validation_data = (X_test_encoded, fold[\"y_test\"]),\n",
    "# #           callbacks = modelCallbacks)\n",
    "# model.fit(x = X_train_encoded, y = fold[\"y_train\"],\n",
    "#           batch_size = batch_size, epochs = 25, verbose = 1,\n",
    "#           validation_data = (X_test_encoded, fold[\"y_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## create the evaluation data structure for all iterations\n",
    "# evaluations = {\n",
    "#     \"Model\" : [],\n",
    "#     \"Dataset\" : [],\n",
    "#     \"Fold\" : [],\n",
    "#     \"AutoEncoder_Categorical_CrossEntropy_Loss\" : [],\n",
    "#     \"AutoEncoder_Reconstruction_Accuracy\" : [],\n",
    "#     \"Train_Test\" : [],\n",
    "#     \"Accuracy\" : [],\n",
    "#     \"Precision\": [],\n",
    "#     \"TPR\": [],\n",
    "#     \"FPR\": [],\n",
    "#     \"TPR_FPR_Thresholds\": [],\n",
    "#     \"AUC\": [],\n",
    "#     \"Sensitivity\": [],\n",
    "#     \"Specificity\": [],\n",
    "#     \"MCC\":[]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_train_encoded)\n",
    "# label_pred = pred2label(y_pred)\n",
    "# # Compute precision, recall, sensitivity, specifity, mcc\n",
    "# acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "# prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "\n",
    "# conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "# if(conf[0][0]+conf[1][0]):\n",
    "#     sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "# else:\n",
    "#     sens = 0.0\n",
    "# if(conf[1][1]+conf[0][1]):\n",
    "#     spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "# else:\n",
    "#     spec = 0.0\n",
    "# if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "#     mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "# else:\n",
    "#     mcc= 0.0\n",
    "# fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "# auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "\n",
    "# evaluations[\"Model\"].append(modelName)\n",
    "# evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "# evaluations[\"Fold\"].append(i)\n",
    "# evaluations[\"Train_Test\"].append(\"Train\")\n",
    "# evaluations[\"Accuracy\"].append(acc)\n",
    "# evaluations[\"Precision\"].append(prec)\n",
    "# evaluations[\"TPR\"].append(tpr)\n",
    "# evaluations[\"FPR\"].append(fpr)\n",
    "# evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "# evaluations[\"AUC\"].append(auc)\n",
    "# evaluations[\"Sensitivity\"].append(sens)\n",
    "# evaluations[\"Specificity\"].append(spec)\n",
    "# evaluations[\"MCC\"].append(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test_encoded)\n",
    "# label_pred = pred2label(y_pred)\n",
    "# # Compute precision, recall, sensitivity, specifity, mcc\n",
    "# acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "# prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "\n",
    "# conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "# if(conf[0][0]+conf[1][0]):\n",
    "#     sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "# else:\n",
    "#     sens = 0.0\n",
    "# if(conf[1][1]+conf[0][1]):\n",
    "#     spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "# else:\n",
    "#     spec = 0.0\n",
    "# if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "#     mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "# else:\n",
    "#     mcc= 0.0\n",
    "# fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "# auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "\n",
    "# evaluations[\"Model\"].append(modelName)\n",
    "# evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "# evaluations[\"Fold\"].append(i)\n",
    "# evaluations[\"Train_Test\"].append(\"Test\")\n",
    "# evaluations[\"Accuracy\"].append(acc)\n",
    "# evaluations[\"Precision\"].append(prec)\n",
    "# evaluations[\"TPR\"].append(tpr)\n",
    "# evaluations[\"FPR\"].append(fpr)\n",
    "# evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "# evaluations[\"AUC\"].append(auc)\n",
    "# evaluations[\"Sensitivity\"].append(sens)\n",
    "# evaluations[\"Specificity\"].append(spec)\n",
    "# evaluations[\"MCC\"].append(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # pd.DataFrame.from_dict(evaluations)\n",
    "# pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in evaluations.items() ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # lr_model = LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "# lr_model = LogisticRegression(fit_intercept=True, max_iter=100)\n",
    "# lr_model.fit(X_train_encoded, \n",
    "#              fold[\"y_train\"].reshape((fold[\"y_train\"].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = lr_model.predict(X_train_encoded)\n",
    "# label_pred = pred2label(y_pred)\n",
    "# # Compute precision, recall, sensitivity, specifity, mcc\n",
    "# acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "# prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "\n",
    "# conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "# if(conf[0][0]+conf[1][0]):\n",
    "#     sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "# else:\n",
    "#     sens = 0.0\n",
    "# if(conf[1][1]+conf[0][1]):\n",
    "#     spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "# else:\n",
    "#     spec = 0.0\n",
    "# if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "#     mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "# else:\n",
    "#     mcc= 0.0\n",
    "# fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "# auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "\n",
    "# # evaluations[\"Model\"].append(modelName)\n",
    "# # evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "# # evaluations[\"Fold\"].append(i)\n",
    "# # evaluations[\"Train_Test\"].append(\"Train\")\n",
    "# # evaluations[\"Accuracy\"].append(acc)\n",
    "# # evaluations[\"Precision\"].append(prec)\n",
    "# # evaluations[\"TPR\"].append(tpr)\n",
    "# # evaluations[\"FPR\"].append(fpr)\n",
    "# # evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "# # evaluations[\"AUC\"].append(auc)\n",
    "# # evaluations[\"Sensitivity\"].append(sens)\n",
    "# # evaluations[\"Specificity\"].append(spec)\n",
    "# # evaluations[\"MCC\"].append(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### For each input file, train model and generate different outputs in a structured folder\n",
    "# ##################################################################################\n",
    "\n",
    "# ## create the evaluation data structure for all iterations\n",
    "# evaluations = {\n",
    "#     \"Model\" : [],\n",
    "#     \"Dataset\" : [],\n",
    "#     \"Fold\" : [],\n",
    "#     \"AutoEncoder_Categorical_CrossEntropy_Loss\" : [],\n",
    "#     \"AutoEncoder_Reconstruction_Accuracy\" : [],\n",
    "#     \"Train_Test\" : [],\n",
    "#     \"Accuracy\" : [],\n",
    "#     \"Precision\": [],\n",
    "#     \"TPR\": [],\n",
    "#     \"FPR\": [],\n",
    "#     \"TPR_FPR_Thresholds\": [],\n",
    "#     \"AUC\": [],\n",
    "#     \"Sensitivity\": [],\n",
    "#     \"Specificity\": [],\n",
    "#     \"MCC\":[]\n",
    "# }\n",
    "\n",
    "# i = 0\n",
    "\n",
    "# for root, dirs, files in os.walk(dataset_setting_path):\n",
    "#     for file in files:\n",
    "        \n",
    "#         current_dataset_variety = root.split(\"\\\\\")[len(root.split(\"\\\\\"))-1]\n",
    "        \n",
    "#         ##################################################################################\n",
    "#         ##### read the current file\n",
    "#         ##################################################################################\n",
    "        \n",
    "#         openFile = open(os.path.join(root, file))\n",
    "#         fastaSequences = SeqIO.parse(openFile, \"fasta\")\n",
    "        \n",
    "#         ##################################################################################\n",
    "#         ##### extract data from the current fasta file\n",
    "#         ##################################################################################\n",
    "        \n",
    "#         nucleosomal_List = []\n",
    "#         linker_List = []\n",
    "#         nucleosomal_onehotencoded_List = []\n",
    "#         linker_onehotencoded_List = []\n",
    "        \n",
    "#         for fasta in fastaSequences: \n",
    "#             name, sequence = fasta.id, str(fasta.seq)\n",
    "#             if \"nucleosomal\" in name:\n",
    "#                 nucleosomal_List.append(sequence)\n",
    "#                 aus_seq = one_hot_encode(sequence)\n",
    "#                 if(len(aus_seq) != 0):\n",
    "#                     nucleosomal_onehotencoded_List.append(aus_seq)\n",
    "#             else:\n",
    "#                 linker_List.append(sequence)\n",
    "#                 aus_seq = one_hot_encode(sequence)\n",
    "#                 if(len(aus_seq) != 0):\n",
    "#                     linker_onehotencoded_List.append(aus_seq)\n",
    "        \n",
    "#         print(\"\\n======================================================================\")\n",
    "#         print(\"\\nFile: \"+os.path.join(root, file))\n",
    "#         print(\"Nucleosomi: \"+str(len(nucleosomal_onehotencoded_List)))\n",
    "#         print(\"Linker: \"+str(len(linker_onehotencoded_List)))\n",
    "        \n",
    "#         ##################################################################################\n",
    "#         ##### Generate Folds from dataset, and store to file\n",
    "#         ##################################################################################\n",
    "        \n",
    "#         ## create the features and labels datasets for the training\n",
    "#         input_size = (len(nucleosomal_onehotencoded_List[1]), 4)\n",
    "#         labels = np.concatenate((np.ones((len(nucleosomal_onehotencoded_List), 1), dtype=np.float32), np.zeros((len(linker_onehotencoded_List), 1), dtype=np.float32)), axis=0)\n",
    "#         features = np.concatenate((nucleosomal_onehotencoded_List,linker_onehotencoded_List), 0)\n",
    "\n",
    "#         ## Generate the k-fold dataset\n",
    "#         folds = build_kfold(features, labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "        \n",
    "#         ## Write the k-fold dataset to file\n",
    "#         foldPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold))\n",
    "        \n",
    "#         ##### ADDITIONAL CHANGES - USE PREVIOUS GENERATED FOLDS IF AVAILABLE\n",
    "        \n",
    "#         if(os.path.isfile(os.path.join(foldPath, foldName))):\n",
    "#             folds = pickle.load(open(os.path.join(foldPath, foldName), \"rb\"))\n",
    "#         else:\n",
    "#             if(not os.path.isdir(foldPath)):\n",
    "#                 os.makedirs(foldPath)\n",
    "#             pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))\n",
    "        \n",
    "#         ##################################################################################\n",
    "#         ##### TRAIN and PREDICT for every Fold, using both models (DLNN3 and DLNN5)\n",
    "#         ##################################################################################\n",
    "        \n",
    "#         for modelName in modelNames:\n",
    "            \n",
    "#             ## Create and set directory to save model\n",
    "#             modelPath = os.path.join(outPath, expName, current_dataset_variety, \"{}fold\".format(n_fold), \"models\", modelName)\n",
    "#             if(not os.path.isdir(modelPath)):\n",
    "#                 os.makedirs(modelPath)\n",
    "            \n",
    "#             ## fold counter\n",
    "#             i = 0\n",
    "\n",
    "#             for fold in folds:\n",
    "                \n",
    "#                 print(\"\\nTrain/Test model \"+modelName+\" on Fold #\"+str(i)+\".\")\n",
    "                \n",
    "#                 ## Generate AUTOENCODER model using function\n",
    "#                 ae_model, encoder = biLSTM_AutoEncoder(input_shape = input_size, lstm_decode_units=150)\n",
    "                \n",
    "#                 ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "#                 modelCallbacks = [\n",
    "#                     tf.keras.callbacks.ModelCheckpoint(os.path.join(modelPath, \"{}_{}_bestModel-fold{}.hdf5\".format(modelName, \"AutoEncoder\", i)),\n",
    "#                                                        monitor = 'val_loss', verbose = 0, save_best_only = True, \n",
    "#                                                        save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "#                     tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 0, \n",
    "#                                                      mode = 'auto', baseline = None, restore_best_weights = False)\n",
    "#                 ]\n",
    "#                 ae_model.fit(x = fold[\"X_train\"], y = fold[\"X_train\"], batch_size = batch_size, epochs = epochs, verbose = 0,\n",
    "#                              callbacks = modelCallbacks, validation_data = (fold[\"X_test\"], fold[\"X_test\"]))\n",
    "                \n",
    "#                 ## dump Encoder to hdf5\n",
    "#                 encoder.save(os.path.join(modelPath, \"{}_{}_bestModel-fold{}.hdf5\".format(modelName, \"Encoder\", i)))\n",
    "                \n",
    "#                 ## Generate embeddings using Encoder model\n",
    "#                 X_train_encoded = encoder.predict(fold['X_train'])\n",
    "#                 X_test_encoded = encoder.predict(fold['X_test'])\n",
    "                \n",
    "#                 X_train_encoded = X_train_encoded.reshape((X_train_encoded.shape[0], X_train_encoded.shape[1]))\n",
    "#                 X_test_encoded = X_test_encoded.reshape((X_test_encoded.shape[0], X_test_encoded.shape[1]))\n",
    "                \n",
    "#                 ## Generate CLASSIFIER model using function\n",
    "#                 model = FFNN_Classifier(input_shape = (X_train_encoded.shape[1]), dense_decode_units = X_train_encoded.shape[1], learn_rate = 0.005)\n",
    "                \n",
    "#                 ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "#                 modelCallbacks = [\n",
    "#                     tf.keras.callbacks.ModelCheckpoint(os.path.join(modelPath, \"{}_{}_bestModel-fold{}.hdf5\".format(modelName, \"Classifier\", i)),\n",
    "#                                                        monitor = 'val_loss', verbose = 0, save_best_only = True, \n",
    "#                                                        save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "#                     tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 0, \n",
    "#                                                      mode = 'auto', baseline = None, restore_best_weights = False)\n",
    "#                 ]\n",
    "#                 model.fit(x = X_train_encoded, y = fold[\"y_train\"],\n",
    "#                           batch_size = batch_size, epochs = 100, verbose = 0,\n",
    "#                           validation_data = (X_test_encoded, fold[\"y_test\"]),\n",
    "#                           callbacks = modelCallbacks)\n",
    "                \n",
    "#                 ##################################################################################\n",
    "#                 ##### Autoencoder metrics for TRAIN dataset\n",
    "#                 ##################################################################################\n",
    "                \n",
    "#                 x_pred = ae_model.predict(fold['X_train'])\n",
    "#                 x_pred_label = pred2label_AE(x_pred)\n",
    "#                 cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "#                 evaluations[\"AutoEncoder_Categorical_CrossEntropy_Loss\"].append(cce(fold['X_train'], x_pred).numpy())\n",
    "#                 evaluations[\"AutoEncoder_Reconstruction_Accuracy\"].append(average_reconstruction_accuracy(fold['X_train'], x_pred_label))\n",
    "                \n",
    "#                 ##################################################################################\n",
    "#                 ##### Prediction and metrics for TRAIN dataset\n",
    "#                 ##################################################################################\n",
    "                \n",
    "#                 y_pred = model.predict(X_train_encoded)\n",
    "#                 label_pred = pred2label(y_pred)\n",
    "#                 # Compute precision, recall, sensitivity, specifity, mcc\n",
    "#                 acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "#                 prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "\n",
    "#                 conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "#                 if(conf[0][0]+conf[1][0]):\n",
    "#                     sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "#                 else:\n",
    "#                     sens = 0.0\n",
    "#                 if(conf[1][1]+conf[0][1]):\n",
    "#                     spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "#                 else:\n",
    "#                     spec = 0.0\n",
    "#                 if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "#                     mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "#                 else:\n",
    "#                     mcc= 0.0\n",
    "#                 fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "#                 auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "\n",
    "#                 evaluations[\"Model\"].append(modelName)\n",
    "#                 evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "#                 evaluations[\"Fold\"].append(i)\n",
    "#                 evaluations[\"Train_Test\"].append(\"Train\")\n",
    "#                 evaluations[\"Accuracy\"].append(acc)\n",
    "#                 evaluations[\"Precision\"].append(prec)\n",
    "#                 evaluations[\"TPR\"].append(tpr)\n",
    "#                 evaluations[\"FPR\"].append(fpr)\n",
    "#                 evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "#                 evaluations[\"AUC\"].append(auc)\n",
    "#                 evaluations[\"Sensitivity\"].append(sens)\n",
    "#                 evaluations[\"Specificity\"].append(spec)\n",
    "#                 evaluations[\"MCC\"].append(mcc)\n",
    "                \n",
    "#                 ##################################################################################\n",
    "#                 ##### Autoencoder metrics for TEST dataset\n",
    "#                 ##################################################################################\n",
    "                \n",
    "#                 x_pred = ae_model.predict(fold['X_test'])\n",
    "#                 x_pred_label = pred2label_AE(x_pred)\n",
    "#                 cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "#                 evaluations[\"AutoEncoder_Categorical_CrossEntropy_Loss\"].append(cce(fold['X_test'], x_pred).numpy())\n",
    "#                 evaluations[\"AutoEncoder_Reconstruction_Accuracy\"].append(average_reconstruction_accuracy(fold['X_test'], x_pred_label))\n",
    "                \n",
    "#                 ##################################################################################\n",
    "#                 ##### Prediction and metrics for TEST dataset\n",
    "#                 ##################################################################################\n",
    "\n",
    "#                 y_pred = model.predict(X_test_encoded)\n",
    "#                 label_pred = pred2label(y_pred)\n",
    "#                 # Compute precision, recall, sensitivity, specifity, mcc\n",
    "#                 acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "#                 prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "\n",
    "#                 conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "#                 if(conf[0][0]+conf[1][0]):\n",
    "#                     sens = float(conf[0][0])/float(conf[0][0]+conf[1][0])\n",
    "#                 else:\n",
    "#                     sens = 0.0\n",
    "#                 if(conf[1][1]+conf[0][1]):\n",
    "#                     spec = float(conf[1][1])/float(conf[1][1]+conf[0][1])\n",
    "#                 else:\n",
    "#                     spec = 0.0\n",
    "#                 if((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0])):\n",
    "#                     mcc = (float(conf[0][0])*float(conf[1][1]) - float(conf[1][0])*float(conf[0][1]))/math.sqrt((conf[0][0]+conf[0][1])*(conf[0][0]+conf[1][0])*(conf[1][1]+conf[0][1])*(conf[1][1]+conf[1][0]))\n",
    "#                 else:\n",
    "#                     mcc= 0.0\n",
    "#                 fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "#                 auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "                \n",
    "#                 evaluations[\"Model\"].append(modelName)\n",
    "#                 evaluations[\"Dataset\"].append(current_dataset_variety)\n",
    "#                 evaluations[\"Fold\"].append(i)\n",
    "#                 evaluations[\"Train_Test\"].append(\"Test\")\n",
    "#                 evaluations[\"Accuracy\"].append(acc)\n",
    "#                 evaluations[\"Precision\"].append(prec)\n",
    "#                 evaluations[\"TPR\"].append(tpr)\n",
    "#                 evaluations[\"FPR\"].append(fpr)\n",
    "#                 evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "#                 evaluations[\"AUC\"].append(auc)\n",
    "#                 evaluations[\"Sensitivity\"].append(sens)\n",
    "#                 evaluations[\"Specificity\"].append(spec)\n",
    "#                 evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "#                 i = i+1\n",
    "#                 del ae_model\n",
    "#                 del model\n",
    "#                 del encoder\n",
    "#                 tf.keras.backend.clear_session()\n",
    "                \n",
    "# ##################################################################################\n",
    "# ##### Dump evaluations to a file\n",
    "# ##################################################################################\n",
    "\n",
    "# evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "# if(not os.path.isdir(evalPath)):\n",
    "#     os.makedirs(evalPath)\n",
    "\n",
    "# pickle.dump(evaluations,\n",
    "#             open(os.path.join(evalPath, \"{}fold_evaluations_{}.pickle\".format(n_fold, modelNames[0])), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Add import statement here, to make this next part of code standalone executable\n",
    "##################################################################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Parameters used only in this section\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 10\n",
    "expName = \"LSTM_AE_Setting1\"\n",
    "outPath = \"Generated\"\n",
    "\n",
    "modelNames = [\"Bi_LSTM_AE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Load file and convert to dataframe for easy manipulation\n",
    "##################################################################################\n",
    "\n",
    "evalPath = os.path.join(outPath, expName, \"_Evaluation_All_Datasets\")\n",
    "if(not os.path.isdir(evalPath)):\n",
    "    os.makedirs(evalPath)\n",
    "\n",
    "evaluations = pickle.load(open(os.path.join(evalPath, \"{}fold_evaluations_{}.pickle\".format(n_fold, modelNames[0])), \"rb\"))\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Group dataset (mean of metrics) by [Dataset, Model, Train_Test] combinations\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Dataset\", \n",
    "                                                 \"Model\", \n",
    "                                                 \"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC',\n",
    "                                                                               'AutoEncoder_Categorical_CrossEntropy_Loss',\n",
    "                                                                               'AutoEncoder_Reconstruction_Accuracy'])\n",
    "\n",
    "Eval_Train = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(2), ['Train'])]\n",
    "Eval_Test = evaluations_df_grouped[np.in1d(evaluations_df_grouped.index.get_level_values(2), ['Test'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Decide on metric to visualize\n",
    "##################################################################################\n",
    "\n",
    "print(\"Metrics Available : \")\n",
    "print(list(evaluations_df_grouped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a metric to plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = \"Accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Visualize with a multiple Bar chart\n",
    "##################################################################################\n",
    "\n",
    "x = np.arange(len(Eval_Train[metric_to_plot]))\n",
    "width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17,6))\n",
    "rects1 = ax.bar(x - (1.5*(width/2)), round(Eval_Train[metric_to_plot]*100, 3), width, label='Bi_LSTM_AE, Train')\n",
    "rects3 = ax.bar(x + (1.5*(width/2)), round(Eval_Test[metric_to_plot]*100, 3), width, label='Bi_LSTM_AE, Test')\n",
    "\n",
    "## Custom y-axis tick labels\n",
    "ax.set_ylabel(metric_to_plot)\n",
    "ax.set_ylim([(math.floor(min(evaluations_df_grouped[metric_to_plot])*10)-1)*10, \n",
    "            (math.ceil(max(evaluations_df_grouped[metric_to_plot])*10)+1)*10])\n",
    "# ax.set_ylim([80, 105])\n",
    "\n",
    "## Custom x-axis tick labels\n",
    "ax.set_xticks(x)\n",
    "# ax.set_xticklabels(DLNN_3_Train.index.get_level_values(0))\n",
    "# ax.set_xticklabels([m+\" - \"+str(n) for m,n in \n",
    "#                         zip(DLNN_CORENup_Train.index.get_level_values(0),DLNN_CORENup_Train.index.get_level_values(1))],\n",
    "#                   rotation=30)\n",
    "ax.set_xticklabels(Eval_Train.index.get_level_values(0),\n",
    "                  rotation=30)\n",
    "\n",
    "\n",
    "ax.set_title(metric_to_plot+' by Dataset, Model, Train/Test')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\", \n",
    "                    ha='center', va='bottom', rotation=90)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all metrics' plots to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Iteratively generate comparison plot using every metric\n",
    "##################################################################################\n",
    "\n",
    "for metric_to_plot in list(evaluations_df_grouped.columns):\n",
    "    \n",
    "    x = np.arange(len(Eval_Train[metric_to_plot]))\n",
    "    width = 0.15\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(17,6))\n",
    "    rects1 = ax.bar(x - (1.5*(width/2)), round(Eval_Train[metric_to_plot]*100, 3), width, label='Bi_LSTM_AE, Train')\n",
    "    rects3 = ax.bar(x + (1.5*(width/2)), round(Eval_Test[metric_to_plot]*100, 3), width, label='Bi_LSTM_AE, Test')\n",
    "\n",
    "    ## Custom y-axis tick labels\n",
    "    ax.set_ylabel(metric_to_plot)\n",
    "    ax.set_ylim([(math.floor(min(evaluations_df_grouped[metric_to_plot])*10)-1)*10, \n",
    "                (math.ceil(max(evaluations_df_grouped[metric_to_plot])*10)+1)*10])\n",
    "    # ax.set_ylim([80, 105])\n",
    "\n",
    "    ## Custom x-axis tick labels\n",
    "    ax.set_xticks(x)\n",
    "    # ax.set_xticklabels(DLNN_3_Train.index.get_level_values(0))\n",
    "    # ax.set_xticklabels([m+\" - \"+str(n) for m,n in \n",
    "    #                         zip(DLNN_CORENup_Train.index.get_level_values(0),DLNN_CORENup_Train.index.get_level_values(1))],\n",
    "    #                   rotation=30)\n",
    "    ax.set_xticklabels(Eval_Train.index.get_level_values(0),\n",
    "                      rotation=30)\n",
    "\n",
    "\n",
    "    ax.set_title(metric_to_plot+' by Dataset, Model, Train/Test')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\", \n",
    "                        ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects3)\n",
    "    \n",
    "    plt.savefig(os.path.join(evalPath, \"{}_{}_Comparison\".format(metric_to_plot, modelNames[0])))\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
