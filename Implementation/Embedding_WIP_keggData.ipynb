{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import os\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "import six.moves.cPickle\n",
    "\n",
    "import r2v_functions as r2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "k = 10\n",
    "\n",
    "# path = \"C:\\\\Users\\\\arya3\\\\Desktop\\\\_Uni\\\\3. CI Project\\\\_Other_Cloned_Repo\\\\16s_embeddings\\\\code\\\\data\\\\\"\n",
    "path = \"embedding_data\"\n",
    "\n",
    "# n_fold = 10\n",
    "# expName = \"Test_Run_current_setting1\"\n",
    "# outPath = \"Generated\"\n",
    "# foldName = \"folds.pickle\"\n",
    "\n",
    "# modelNames = [\"DLNN_CORENup\"]\n",
    "\n",
    "# epochs = 200\n",
    "# batch_size = 64\n",
    "# shuffle = False\n",
    "# seed = None\n",
    "\n",
    "# dataset_path = \"CORENup-Datasets\\\\Datasets\"\n",
    "# setting = \"Setting1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read fasta file\n",
    "subset_openFile = open(os.path.join(path, \"input\", \"kegg_subset.fasta\"))\n",
    "subset_fastaSequences = SeqIO.parse(subset_openFile, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### extract data from the current fasta file\n",
    "##################################################################################\n",
    "\n",
    "subset_id_List = []\n",
    "subset_seq_List = []\n",
    "\n",
    "for subset_fasta in subset_fastaSequences:\n",
    "    \n",
    "    name, sequence = subset_fasta.id, str(subset_fasta.seq)\n",
    "    \n",
    "    subset_id_List.append(name)\n",
    "    subset_seq_List.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Generate k-mers and write to file\n",
    "##################################################################################\n",
    "\n",
    "out_subset_kmers = gzip.open(os.path.join(path, \"output\", \"kegg_subset_model_input.gz\"),'w')\n",
    "\n",
    "subset_kmers = []\n",
    "for subset_sequence in subset_seq_List:\n",
    "    curr_seq_kmers = []\n",
    "    for i in range(0,len(subset_seq_List[0]) - k + 1):\n",
    "        curr_seq_kmers.append(sequence[i:i+k])\n",
    "    subset_kmers.append(curr_seq_kmers)\n",
    "    \n",
    "    curr_seq_kmers_joined = \" \".join(map(str, subset_kmers[0]))+\"\\n\"\n",
    "    out_subset_kmers.write(curr_seq_kmers_joined.encode())\n",
    "\n",
    "out_subset_kmers.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on stored file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Model Training parameters\n",
    "##################################################################################\n",
    "\n",
    "seed = random.randint(1,9999999)\n",
    "d = 64\n",
    "w = 50\n",
    "neg_samps = 10\n",
    "samp_freq = 0.0001\n",
    "n_min = 100\n",
    "epochs = 3\n",
    "n_cores = 1\n",
    "work_dir = 0\n",
    "prefix = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = prefix + '_' + str(k) + '_' + str(d) + \\\n",
    "        '_' + str(epochs) + '_' + str(w) + '_' + \\\n",
    "        str(neg_samps).replace('0.','') + '_' + \\\n",
    "        str(samp_freq) + '_' + str(n_min) + '_model.pkl'\n",
    "\n",
    "model_path = os.path.join(path, \"output\", model_fn)\n",
    "\n",
    "kmers_path = os.path.join(path, \"output\", \"kegg_subset_model_input.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers_init = LineSentence(kmers_path, max_sentence_length=100000)\n",
    "\n",
    "model = Word2Vec(kmers_init,sg=1,size=d,window=w,min_count=n_min,negative=neg_samps,\n",
    "                 sample=samp_freq,iter=epochs,workers=n_cores,seed=seed)\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Needed Parameters \n",
    "##################################################################################\n",
    "\n",
    "nr = bool(int(1))\n",
    "a = 1e-05\n",
    "v = 1000\n",
    "\n",
    "path_reads = os.path.join(path, \"input\", \"kegg_subset.fasta.gz\")\n",
    "path_model = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_totalkmers = '%s_%s_totalkmers.pkl' % (prefix,str(k))\n",
    "path_totalkmers = os.path.join(path, \"output\", fn_totalkmers)\n",
    "\n",
    "work_dir = os.path.join(path, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing read 0. Last batch: 0.000 minutes. Total time: 0.000 hours.\n",
      "Processing read 1000. Last batch: 0.095 minutes. Total time: 0.002 hours.\n",
      "Processing read 2000. Last batch: 0.096 minutes. Total time: 0.003 hours.\n",
      "Loading total kmers.\n",
      "Loading model.\n",
      "Total reads in sample kegg_subset: 2500.\n",
      "Normalizing each read by total number of kmers in that read.\n",
      "Processing eco:b020: 0/2500.\n",
      "Processing esc:Entcl_R007: 1000/2500.\n",
      "Processing sfw:WN53_1327: 2000/2500.\n",
      "Saving reads to embedding_data\\output\\kegg_subset_10_64_3_50_10_0.0001_100_1e-05_remb_raw.csv.gz.\n",
      "Performing SVD: (64,2500).\n",
      "Saving reads to embedding_data\\output\\kegg_subset_10_64_3_50_10_0.0001_100_1e-05_remb.csv.gz.\n"
     ]
    }
   ],
   "source": [
    "total_kmers = r2v.calc_total_kmers(path_reads, path_model, k, verbose=True, v=v)\n",
    "\n",
    "six.moves.cPickle.dump(total_kmers, open(path_totalkmers, 'wb'), protocol=4)\n",
    "\n",
    "r2v.embed_reads(path_sample = path_reads, path_totalkmers = path_totalkmers, path_model = path_model, path_out = work_dir, normread=nr, k=k, a=a, verbose=True, v=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## read fasta file\n",
    "# subset_openFile = open(os.path.join(path, \"input\", \"hmp_subset.fasta\"))\n",
    "# subset_fastaSequences = SeqIO.parse(subset_openFile, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### extract data from the current fasta file\n",
    "# ##################################################################################\n",
    "\n",
    "# subset_id_List = []\n",
    "# subset_seq_List = []\n",
    "\n",
    "# for subset_fasta in subset_fastaSequences:\n",
    "    \n",
    "#     name, sequence = subset_fasta.id, str(subset_fasta.seq)\n",
    "    \n",
    "#     subset_id_List.append(name)\n",
    "#     subset_seq_List.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations = pickle.load(open(os.path.join(path, \"output\", \"exread2_10_totalkmers.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(evaluations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
